{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82b6d2d",
   "metadata": {},
   "source": [
    "# ðŸ¤– The Autonomous Colony - GPU Training\n",
    "\n",
    "Train RL agents with GPU acceleration on Google Colab.\n",
    "\n",
    "**Quick Start:**\n",
    "1. Runtime â†’ Change runtime type â†’ **GPU (T4)**\n",
    "2. Run all cells in order\n",
    "3. Models auto-save to Google Drive\n",
    "4. Download and visualize locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895840a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for model persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "MODEL_DIR = '/content/drive/MyDrive/autonomous_colony_models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"âœ“ Models will be saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62cd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ritikkumarv/autonomous-colony.git\n",
    "%cd autonomous-colony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd46ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numpy matplotlib seaborn\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fd877",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ece658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'n_agents': 2,\n",
    "    'grid_size': 20,\n",
    "    'n_episodes': 500,\n",
    "    'max_steps': 200,\n",
    "    'save_interval': 100,\n",
    "    'agent_type': 'ppo',  # 'ppo' or 'dqn'\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3fd6c",
   "metadata": {},
   "source": [
    "## Initialize Agent & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/autonomous-colony')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "\n",
    "from src.environment import ColonyEnvironment\n",
    "from src.agents import PPOAgent, DQNAgent\n",
    "\n",
    "# Create environment\n",
    "env = ColonyEnvironment(\n",
    "    n_agents=CONFIG['n_agents'],\n",
    "    grid_size=CONFIG['grid_size']\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "if CONFIG['agent_type'] == 'ppo':\n",
    "    agent = PPOAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=3e-4,\n",
    "        n_epochs=10,\n",
    "        batch_size=64\n",
    "    )\n",
    "else:  # dqn\n",
    "    agent = DQNAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        buffer_size=50000\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ“ {CONFIG['agent_type'].upper()} Agent initialized\")\n",
    "print(f\"âœ“ Device: {agent.device}\")\n",
    "print(f\"âœ“ Environment: {CONFIG['grid_size']}x{CONFIG['grid_size']} grid, {CONFIG['n_agents']} agents\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ff5b2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "success_rates = []\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"\\nStarting Training...\\n\")\n",
    "\n",
    "for episode in range(CONFIG['n_episodes']):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Episode loop\n",
    "    while not done and step < CONFIG['max_steps']:\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        \n",
    "        # Select actions for all agents\n",
    "        for obs in observations:\n",
    "            if CONFIG['agent_type'] == 'ppo':\n",
    "                action, log_prob, value = agent.select_action(obs, training=True)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "            else:  # DQN\n",
    "                action = agent.select_action(obs, training=True)\n",
    "            \n",
    "            actions.append(action)\n",
    "        \n",
    "        # Step environment\n",
    "        next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "        # Store transitions\n",
    "        if CONFIG['agent_type'] == 'ppo':\n",
    "            # Store each agent's transition using PPO's store_transition method\n",
    "            for i, (obs, action, reward, done_flag) in enumerate(zip(observations, actions, rewards, dones)):\n",
    "                agent.store_transition(\n",
    "                    state=obs,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    log_prob=log_probs[i],\n",
    "                    value=values[i],\n",
    "                    done=done_flag\n",
    "                )\n",
    "        else:  # DQN\n",
    "            for obs, action, reward, next_obs, done_flag in zip(\n",
    "                observations, actions, rewards, next_observations, dones\n",
    "            ):\n",
    "                agent.memory.push(obs, action, reward, next_obs, done_flag)\n",
    "        \n",
    "        episode_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated[0] or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    # Update agent\n",
    "    if CONFIG['agent_type'] == 'ppo':\n",
    "        if len(agent.rollout_buffer) >= agent.batch_size:\n",
    "            loss = agent.update()\n",
    "    else:  # DQN\n",
    "        if len(agent.memory) >= agent.batch_size:\n",
    "            loss = agent.update()\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step)\n",
    "    success_rates.append(1 if episode_reward > 0 else 0)\n",
    "    \n",
    "    # Progress logging and visualization\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # Create plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Episode rewards\n",
    "        axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "        if len(episode_rewards) >= 50:\n",
    "            ma = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "            axes[0, 0].plot(range(49, len(episode_rewards)), ma, 'r-', linewidth=2, label='MA(50)')\n",
    "        axes[0, 0].set_title('Episode Rewards')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Total Reward')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        axes[0, 1].plot(episode_lengths, alpha=0.6, color='green')\n",
    "        axes[0, 1].set_title('Episode Lengths')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Steps')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Success rate\n",
    "        axes[1, 0].clear()\n",
    "        if len(success_rates) >= 50:\n",
    "            sr = np.convolve(success_rates, np.ones(50)/50, mode='valid')\n",
    "            axes[1, 0].plot(range(49, len(success_rates)), sr, 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Success Rate (50-ep moving avg)')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Success Rate')\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Reward distribution\n",
    "        recent = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
    "        axes[1, 1].hist(recent, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[1, 1].set_title('Recent Reward Distribution (last 100)')\n",
    "        axes[1, 1].set_xlabel('Reward')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print stats\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        avg_length = np.mean(episode_lengths[-100:])\n",
    "        success_rate = np.mean(success_rates[-100:])\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{CONFIG['n_episodes']}\")\n",
    "        print(f\"  Current Reward: {episode_reward:.2f}\")\n",
    "        print(f\"  Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "        print(f\"  Avg Length (last 100): {avg_length:.1f}\")\n",
    "        print(f\"  Success Rate (last 100): {success_rate:.1%}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % CONFIG['save_interval'] == 0:\n",
    "        checkpoint_path = f\"{MODEL_DIR}/{CONFIG['agent_type']}_ep{episode+1}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'episode': episode + 1,\n",
    "            'config': CONFIG,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'success_rates': success_rates,\n",
    "        }\n",
    "        \n",
    "        if CONFIG['agent_type'] == 'ppo':\n",
    "            checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        else:  # DQN\n",
    "            checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "            checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"\\nðŸ’¾ Checkpoint saved: {checkpoint_path}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Avg Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Final Success Rate (last 100): {np.mean(success_rates[-100:]):.1%}\")\n",
    "print(f\"Best Episode Reward: {max(episode_rewards):.2f}\")\n",
    "print(f\"Models saved to: {MODEL_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320d6a8",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83232263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final trained model\n",
    "final_path = f\"{MODEL_DIR}/{CONFIG['agent_type']}_final_{timestamp}.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    'episode': CONFIG['n_episodes'],\n",
    "    'config': CONFIG,\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'episode_lengths': episode_lengths,\n",
    "    'success_rates': success_rates,\n",
    "    'final_stats': {\n",
    "        'avg_reward': np.mean(episode_rewards[-100:]),\n",
    "        'success_rate': np.mean(success_rates[-100:]),\n",
    "        'best_reward': max(episode_rewards)\n",
    "    }\n",
    "}\n",
    "\n",
    "if CONFIG['agent_type'] == 'ppo':\n",
    "    checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "else:  # DQN\n",
    "    checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "    checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "\n",
    "torch.save(checkpoint, final_path)\n",
    "\n",
    "print(f\"âœ… Final model saved: {final_path}\")\n",
    "print(f\"\\nTo visualize locally:\")\n",
    "print(f\"1. Download model from Google Drive: {MODEL_DIR}\")\n",
    "print(f\"2. Place in local models/ directory\")\n",
    "print(f\"3. Run: python visualize.py --model models/{CONFIG['agent_type']}_final_{timestamp}.pt --episodes 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb16f2",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f700cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved models\n",
    "models = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pt')]\n",
    "models.sort()\n",
    "\n",
    "print(f\"Saved Models ({len(models)}):\")\n",
    "print(\"=\"*80)\n",
    "for i, model in enumerate(models, 1):\n",
    "    path = os.path.join(MODEL_DIR, model)\n",
    "    size = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"{i}. {model} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ Download from: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for easy download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_name = f\"colony_models_{timestamp}\"\n",
    "zip_path = f\"/content/{zip_name}\"\n",
    "\n",
    "shutil.make_archive(zip_path, 'zip', MODEL_DIR)\n",
    "print(f\"âœ… Created: {zip_path}.zip\")\n",
    "print(\"\\nDownloading...\")\n",
    "\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65440e05",
   "metadata": {},
   "source": [
    "## Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6db615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of trained agent\n",
    "print(\"Testing trained agent...\\n\")\n",
    "\n",
    "test_rewards = []\n",
    "test_lengths = []\n",
    "\n",
    "for test_ep in range(5):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while not done and step < 200:\n",
    "        actions = []\n",
    "        \n",
    "        for obs in observations:\n",
    "            if CONFIG['agent_type'] == 'ppo':\n",
    "                action, _, _ = agent.select_action(obs, training=False)\n",
    "            else:  # DQN\n",
    "                action = agent.select_action(obs, training=False)\n",
    "            actions.append(action)\n",
    "        \n",
    "        next_observations, rewards, dones, truncated, _ = env.step(actions)\n",
    "        ep_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated[0] or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    test_rewards.append(ep_reward)\n",
    "    test_lengths.append(step)\n",
    "    print(f\"Test Episode {test_ep + 1}: Reward={ep_reward:.2f}, Steps={step}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Avg Reward: {np.mean(test_rewards):.2f} Â± {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Avg Length: {np.mean(test_lengths):.1f} Â± {np.std(test_lengths):.1f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c359c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# THE AUTONOMOUS COLONY - PART 1: ENVIRONMENT\n",
    "# A comprehensive RL learning project covering all major concepts\n",
    "# ============================================================================\n",
    "# Run this in Google Colab - No setup needed!\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "# üåç The Autonomous Colony - Environment Foundation\n",
    "\n",
    "This notebook builds a custom Gymnasium environment for multi-agent RL.\n",
    "\n",
    "**Key RL Concepts Covered:**\n",
    "- MDP formulation (States, Actions, Rewards, Transitions)\n",
    "- Partial observability (POMDP)\n",
    "- Sparse rewards & reward shaping\n",
    "- Resource management dynamics\n",
    "- Multi-agent interaction space\n",
    "\n",
    "**Environment Features:**\n",
    "- 2D grid world with resources (food, water, materials)\n",
    "- Multiple agents with energy/health systems\n",
    "- Seasonal changes (dynamic rules)\n",
    "- Partial observations (local vision)\n",
    "- Emergent cooperation/competition\n",
    "\n",
    "**Next Steps:**\n",
    "- Part 2: Tabular Q-learning agent\n",
    "- Part 3: Deep RL (DQN, PPO)\n",
    "- Part 4: Multi-agent coordination\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP - Install Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment to install (first run only)\n",
    "# !pip install gymnasium numpy matplotlib seaborn tensorboard wandb -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì Dependencies loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTANTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class ActionType(IntEnum):\n",
    "    \"\"\"Agent action space - 9 discrete actions\"\"\"\n",
    "    MOVE_UP = 0\n",
    "    MOVE_DOWN = 1\n",
    "    MOVE_LEFT = 2\n",
    "    MOVE_RIGHT = 3\n",
    "    MOVE_UP_LEFT = 4\n",
    "    MOVE_UP_RIGHT = 5\n",
    "    MOVE_DOWN_LEFT = 6\n",
    "    MOVE_DOWN_RIGHT = 7\n",
    "    COLLECT = 8  # Collect resource at current position\n",
    "    # Future: BUILD, COMMUNICATE, REST\n",
    "\n",
    "class ResourceType(IntEnum):\n",
    "    \"\"\"Resources in the environment\"\"\"\n",
    "    EMPTY = 0\n",
    "    FOOD = 1\n",
    "    WATER = 2\n",
    "    MATERIAL = 3\n",
    "    OBSTACLE = 4\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Individual agent state\"\"\"\n",
    "    id: int\n",
    "    x: int\n",
    "    y: int\n",
    "    energy: float = 100.0\n",
    "    health: float = 100.0\n",
    "    food_carried: int = 0\n",
    "    water_carried: int = 0\n",
    "    material_carried: int = 0\n",
    "    steps_alive: int = 0\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentConfig:\n",
    "    \"\"\"Environment hyperparameters\"\"\"\n",
    "    grid_size: int = 20\n",
    "    n_agents: int = 3\n",
    "    max_steps: int = 500\n",
    "    \n",
    "    # Resource spawning\n",
    "    food_spawn_rate: float = 0.02\n",
    "    water_spawn_rate: float = 0.015\n",
    "    material_spawn_rate: float = 0.01\n",
    "    obstacle_density: float = 0.1\n",
    "    \n",
    "    # Agent parameters\n",
    "    initial_energy: float = 100.0\n",
    "    initial_health: float = 100.0\n",
    "    energy_decay: float = 0.1  # Per step\n",
    "    energy_from_food: float = 30.0\n",
    "    health_from_water: float = 20.0\n",
    "    \n",
    "    # Observation\n",
    "    observation_radius: int = 3  # 7x7 local view\n",
    "    \n",
    "    # Rewards\n",
    "    reward_survive: float = 0.1\n",
    "    reward_collect_food: float = 5.0\n",
    "    reward_collect_water: float = 3.0\n",
    "    reward_death_penalty: float = -50.0\n",
    "    reward_low_energy: float = -1.0\n",
    "\n",
    "# ============================================================================\n",
    "# COLONY ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "class ColonyEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium environment for The Autonomous Colony.\n",
    "    \n",
    "    Observation Space (per agent):\n",
    "        - Local grid view: (2*r+1, 2*r+1, n_channels)\n",
    "        - Internal state: [energy, health, food, water, material]\n",
    "        \n",
    "    Action Space:\n",
    "        - Discrete(9): 8 directions + collect\n",
    "        \n",
    "    Reward:\n",
    "        - Survival bonus per step\n",
    "        - Resource collection rewards\n",
    "        - Energy/health penalties\n",
    "        - Death penalty\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
    "    \n",
    "    def __init__(self, config: EnvironmentConfig = None):\n",
    "        super().__init__()\n",
    "        self.config = config or EnvironmentConfig()\n",
    "        \n",
    "        # Grid: stores ResourceType values\n",
    "        self.grid = np.zeros((self.config.grid_size, self.config.grid_size), dtype=np.int32)\n",
    "        \n",
    "        # Agents\n",
    "        self.agents: List[AgentState] = []\n",
    "        self.active_agents: List[bool] = []\n",
    "        \n",
    "        # Season system (for meta-RL later)\n",
    "        self.season = 0  # 0=normal, 1=winter, 2=drought, etc.\n",
    "        \n",
    "        # Spaces\n",
    "        obs_radius = self.config.observation_radius\n",
    "        local_view_size = 2 * obs_radius + 1\n",
    "        \n",
    "        # Observation: local grid (one-hot encoded) + internal state\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'grid': spaces.Box(\n",
    "                low=0, high=1, \n",
    "                shape=(local_view_size, local_view_size, len(ResourceType)),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'state': spaces.Box(\n",
    "                low=0, high=np.inf,\n",
    "                shape=(5,),  # energy, health, food, water, material\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(ActionType))\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Clear grid\n",
    "        self.grid.fill(ResourceType.EMPTY)\n",
    "        \n",
    "        # Place obstacles\n",
    "        n_obstacles = int(self.config.obstacle_density * self.grid.size)\n",
    "        obstacle_positions = self.np_random.choice(\n",
    "            self.grid.size, size=n_obstacles, replace=False\n",
    "        )\n",
    "        obstacle_coords = np.unravel_index(obstacle_positions, self.grid.shape)\n",
    "        self.grid[obstacle_coords] = ResourceType.OBSTACLE\n",
    "        \n",
    "        # Spawn initial resources\n",
    "        self._spawn_resources()\n",
    "        \n",
    "        # Initialize agents at random positions\n",
    "        self.agents = []\n",
    "        self.active_agents = []\n",
    "        \n",
    "        for i in range(self.config.n_agents):\n",
    "            # Find empty position\n",
    "            while True:\n",
    "                x = self.np_random.integers(0, self.config.grid_size)\n",
    "                y = self.np_random.integers(0, self.config.grid_size)\n",
    "                if self.grid[y, x] == ResourceType.EMPTY and not self._position_occupied(x, y):\n",
    "                    break\n",
    "            \n",
    "            agent = AgentState(\n",
    "                id=i,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                energy=self.config.initial_energy,\n",
    "                health=self.config.initial_health\n",
    "            )\n",
    "            self.agents.append(agent)\n",
    "            self.active_agents.append(True)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.episode_rewards = [0.0] * self.config.n_agents\n",
    "        \n",
    "        # Return observations for all agents\n",
    "        observations = [self._get_observation(i) for i in range(self.config.n_agents)]\n",
    "        info = {'step': 0, 'active_agents': sum(self.active_agents)}\n",
    "        \n",
    "        return observations, info\n",
    "    \n",
    "    def step(self, actions: List[int]):\n",
    "        \"\"\"Execute one step with actions from all agents\"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        rewards = [0.0] * self.config.n_agents\n",
    "        dones = [False] * self.config.n_agents\n",
    "        truncated = [False] * self.config.n_agents\n",
    "        \n",
    "        # Process each agent's action\n",
    "        for i, action in enumerate(actions):\n",
    "            if not self.active_agents[i]:\n",
    "                continue\n",
    "                \n",
    "            agent = self.agents[i]\n",
    "            reward = 0.0\n",
    "            \n",
    "            # Execute action\n",
    "            if action <= ActionType.MOVE_DOWN_RIGHT:\n",
    "                reward += self._handle_move(agent, action)\n",
    "            elif action == ActionType.COLLECT:\n",
    "                reward += self._handle_collect(agent)\n",
    "            \n",
    "            # Energy decay\n",
    "            agent.energy -= self.config.energy_decay\n",
    "            \n",
    "            # Survival bonus\n",
    "            reward += self.config.reward_survive\n",
    "            \n",
    "            # Check energy/health\n",
    "            if agent.energy < 20:\n",
    "                reward += self.config.reward_low_energy\n",
    "            \n",
    "            if agent.energy <= 0 or agent.health <= 0:\n",
    "                self.active_agents[i] = False\n",
    "                dones[i] = True\n",
    "                reward += self.config.reward_death_penalty\n",
    "            \n",
    "            agent.steps_alive += 1\n",
    "            rewards[i] = reward\n",
    "            self.episode_rewards[i] += reward\n",
    "        \n",
    "        # Spawn new resources\n",
    "        if self.current_step % 10 == 0:\n",
    "            self._spawn_resources()\n",
    "        \n",
    "        # Check episode termination\n",
    "        episode_done = (self.current_step >= self.config.max_steps) or all(not a for a in self.active_agents)\n",
    "        if episode_done:\n",
    "            truncated = [True] * self.config.n_agents\n",
    "        \n",
    "        # Get observations\n",
    "        observations = [self._get_observation(i) for i in range(self.config.n_agents)]\n",
    "        \n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'active_agents': sum(self.active_agents),\n",
    "            'episode_rewards': self.episode_rewards.copy()\n",
    "        }\n",
    "        \n",
    "        return observations, rewards, dones, truncated, info\n",
    "    \n",
    "    def _handle_move(self, agent: AgentState, action: int) -> float:\n",
    "        \"\"\"Handle movement action\"\"\"\n",
    "        dx, dy = 0, 0\n",
    "        \n",
    "        if action == ActionType.MOVE_UP:\n",
    "            dy = -1\n",
    "        elif action == ActionType.MOVE_DOWN:\n",
    "            dy = 1\n",
    "        elif action == ActionType.MOVE_LEFT:\n",
    "            dx = -1\n",
    "        elif action == ActionType.MOVE_RIGHT:\n",
    "            dx = 1\n",
    "        elif action == ActionType.MOVE_UP_LEFT:\n",
    "            dx, dy = -1, -1\n",
    "        elif action == ActionType.MOVE_UP_RIGHT:\n",
    "            dx, dy = 1, -1\n",
    "        elif action == ActionType.MOVE_DOWN_LEFT:\n",
    "            dx, dy = -1, 1\n",
    "        elif action == ActionType.MOVE_DOWN_RIGHT:\n",
    "            dx, dy = 1, 1\n",
    "        \n",
    "        new_x = np.clip(agent.x + dx, 0, self.config.grid_size - 1)\n",
    "        new_y = np.clip(agent.y + dy, 0, self.config.grid_size - 1)\n",
    "        \n",
    "        # Check if position is valid\n",
    "        if self.grid[new_y, new_x] != ResourceType.OBSTACLE and not self._position_occupied(new_x, new_y, exclude_agent=agent.id):\n",
    "            agent.x = new_x\n",
    "            agent.y = new_y\n",
    "            return 0.0  # No penalty for valid move\n",
    "        else:\n",
    "            return -0.5  # Small penalty for invalid move\n",
    "    \n",
    "    def _handle_collect(self, agent: AgentState) -> float:\n",
    "        \"\"\"Handle resource collection\"\"\"\n",
    "        resource = self.grid[agent.y, agent.x]\n",
    "        \n",
    "        if resource == ResourceType.FOOD:\n",
    "            agent.food_carried += 1\n",
    "            agent.energy = min(agent.energy + self.config.energy_from_food, 100.0)\n",
    "            self.grid[agent.y, agent.x] = ResourceType.EMPTY\n",
    "            return self.config.reward_collect_food\n",
    "        \n",
    "        elif resource == ResourceType.WATER:\n",
    "            agent.water_carried += 1\n",
    "            agent.health = min(agent.health + self.config.health_from_water, 100.0)\n",
    "            self.grid[agent.y, agent.x] = ResourceType.EMPTY\n",
    "            return self.config.reward_collect_water\n",
    "        \n",
    "        elif resource == ResourceType.MATERIAL:\n",
    "            agent.material_carried += 1\n",
    "            self.grid[agent.y, agent.x] = ResourceType.EMPTY\n",
    "            return 1.0  # Small reward for materials\n",
    "        \n",
    "        return -0.1  # Penalty for collecting nothing\n",
    "    \n",
    "    def _get_observation(self, agent_id: int) -> Dict:\n",
    "        \"\"\"Get observation for a specific agent (POMDP - partial observability)\"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "        r = self.config.observation_radius\n",
    "        \n",
    "        # Extract local view\n",
    "        y_min = max(0, agent.y - r)\n",
    "        y_max = min(self.config.grid_size, agent.y + r + 1)\n",
    "        x_min = max(0, agent.x - r)\n",
    "        x_max = min(self.config.grid_size, agent.x + r + 1)\n",
    "        \n",
    "        local_grid = self.grid[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        # Pad if at edge\n",
    "        pad_top = r - (agent.y - y_min)\n",
    "        pad_bottom = r - (y_max - agent.y - 1)\n",
    "        pad_left = r - (agent.x - x_min)\n",
    "        pad_right = r - (x_max - agent.x - 1)\n",
    "        \n",
    "        local_grid = np.pad(local_grid, ((pad_top, pad_bottom), (pad_left, pad_right)), constant_values=ResourceType.OBSTACLE)\n",
    "        \n",
    "        # One-hot encode\n",
    "        grid_one_hot = np.eye(len(ResourceType))[local_grid].astype(np.float32)\n",
    "        \n",
    "        # Internal state\n",
    "        state = np.array([\n",
    "            agent.energy / 100.0,\n",
    "            agent.health / 100.0,\n",
    "            agent.food_carried / 10.0,  # Normalize\n",
    "            agent.water_carried / 10.0,\n",
    "            agent.material_carried / 10.0\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return {'grid': grid_one_hot, 'state': state}\n",
    "    \n",
    "    def _spawn_resources(self):\n",
    "        \"\"\"Spawn resources randomly\"\"\"\n",
    "        empty_positions = np.argwhere(self.grid == ResourceType.EMPTY)\n",
    "        \n",
    "        if len(empty_positions) == 0:\n",
    "            return\n",
    "        \n",
    "        # Food\n",
    "        n_food = int(self.config.food_spawn_rate * len(empty_positions))\n",
    "        if n_food > 0:\n",
    "            food_indices = self.np_random.choice(len(empty_positions), size=min(n_food, len(empty_positions)), replace=False)\n",
    "            for idx in food_indices:\n",
    "                y, x = empty_positions[idx]\n",
    "                self.grid[y, x] = ResourceType.FOOD\n",
    "        \n",
    "        # Water\n",
    "        empty_positions = np.argwhere(self.grid == ResourceType.EMPTY)\n",
    "        n_water = int(self.config.water_spawn_rate * len(empty_positions))\n",
    "        if n_water > 0 and len(empty_positions) > 0:\n",
    "            water_indices = self.np_random.choice(len(empty_positions), size=min(n_water, len(empty_positions)), replace=False)\n",
    "            for idx in water_indices:\n",
    "                y, x = empty_positions[idx]\n",
    "                self.grid[y, x] = ResourceType.WATER\n",
    "    \n",
    "    def _position_occupied(self, x: int, y: int, exclude_agent: int = -1) -> bool:\n",
    "        \"\"\"Check if position is occupied by another agent\"\"\"\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i != exclude_agent and self.active_agents[i]:\n",
    "                if agent.x == x and agent.y == y:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # Grid visualization\n",
    "        grid_vis = np.zeros((*self.grid.shape, 3))\n",
    "        \n",
    "        # Color mapping\n",
    "        colors = {\n",
    "            ResourceType.EMPTY: [0.1, 0.1, 0.1],\n",
    "            ResourceType.FOOD: [0.2, 0.8, 0.2],\n",
    "            ResourceType.WATER: [0.2, 0.5, 1.0],\n",
    "            ResourceType.MATERIAL: [0.6, 0.4, 0.2],\n",
    "            ResourceType.OBSTACLE: [0.3, 0.3, 0.3]\n",
    "        }\n",
    "        \n",
    "        for resource_type, color in colors.items():\n",
    "            mask = self.grid == resource_type\n",
    "            grid_vis[mask] = color\n",
    "        \n",
    "        ax1.imshow(grid_vis)\n",
    "        \n",
    "        # Draw agents\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if self.active_agents[i]:\n",
    "                color = plt.cm.Set1(i / len(self.agents))\n",
    "                ax1.plot(agent.x, agent.y, 'o', markersize=15, color=color, markeredgecolor='white', markeredgewidth=2)\n",
    "                ax1.text(agent.x, agent.y, str(i), ha='center', va='center', color='white', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax1.set_title(f'Colony World (Step {self.current_step})', fontsize=14, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Agent stats\n",
    "        ax2.axis('off')\n",
    "        stats_text = f\"COLONY STATUS - Step {self.current_step}/{self.config.max_steps}\\n\\n\"\n",
    "        \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            status = \"ACTIVE\" if self.active_agents[i] else \"DEAD\"\n",
    "            stats_text += f\"Agent {i} [{status}]\\n\"\n",
    "            stats_text += f\"  Position: ({agent.x}, {agent.y})\\n\"\n",
    "            stats_text += f\"  Energy: {agent.energy:.1f}/100\\n\"\n",
    "            stats_text += f\"  Health: {agent.health:.1f}/100\\n\"\n",
    "            stats_text += f\"  Food: {agent.food_carried}, Water: {agent.water_carried}\\n\"\n",
    "            stats_text += f\"  Reward: {self.episode_rewards[i]:.1f}\\n\\n\"\n",
    "        \n",
    "        stats_text += f\"\\nActive Agents: {sum(self.active_agents)}/{len(self.agents)}\"\n",
    "        \n",
    "        ax2.text(0.1, 0.9, stats_text, fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING & DEMO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING COLONY ENVIRONMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create environment\n",
    "config = EnvironmentConfig(\n",
    "    grid_size=15,\n",
    "    n_agents=3,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "env = ColonyEnvironment(config)\n",
    "print(f\"‚úì Environment created: {config.grid_size}x{config.grid_size} grid, {config.n_agents} agents\")\n",
    "\n",
    "# Test reset\n",
    "observations, info = env.reset(seed=42)\n",
    "print(f\"‚úì Environment reset: {len(observations)} observations\")\n",
    "print(f\"  Observation shape - Grid: {observations[0]['grid'].shape}, State: {observations[0]['state'].shape}\")\n",
    "\n",
    "# Random rollout\n",
    "print(\"\\nüéÆ Running random policy for 50 steps...\")\n",
    "env.render()\n",
    "\n",
    "for step in range(50):\n",
    "    # Random actions for all agents\n",
    "    actions = [env.action_space.sample() for _ in range(config.n_agents)]\n",
    "    observations, rewards, dones, truncated, info = env.step(actions)\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        print(f\"\\nStep {step}: Active agents = {info['active_agents']}, Rewards = {[f'{r:.1f}' for r in rewards]}\")\n",
    "        env.render()\n",
    "    \n",
    "    if all(truncated):\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úì Episode complete! Final rewards: {info['episode_rewards']}\")\n",
    "print(f\"  Survival times: {[agent.steps_alive for agent in env.agents]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENVIRONMENT READY! Next: Implement RL agents\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a660bca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# THE AUTONOMOUS COLONY - PART 4: ADVANCED RL CONCEPTS\n",
    "# Meta-Learning, World Models, Hierarchical RL, and Curiosity\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "# ðŸ§  The Autonomous Colony - Advanced RL\n",
    "\n",
    "**RL Concepts Covered:**\n",
    "1. Meta-RL / Learning to Learn (MAML-style)\n",
    "2. World Models (simplified DreamerV3)\n",
    "3. Hierarchical RL (Options Framework)\n",
    "4. Intrinsic Motivation (Curiosity/ICM)\n",
    "5. Offline RL / Imitation Learning\n",
    "6. Curriculum Learning\n",
    "7. Model-based planning\n",
    "\n",
    "**Prerequisites:**\n",
    "- Parts 1-3 (environment, agents, multi-agent)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# !pip install torch numpy matplotlib -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "print(\"âœ“ Advanced RL modules loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. WORLD MODEL (Model-Based RL)\n",
    "# ============================================================================\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified world model for planning.\n",
    "    Learns to predict next state and reward given current state and action.\n",
    "    \n",
    "    RL Concepts:\n",
    "    - Model-based RL: learn dynamics model\n",
    "    - Planning: use model to simulate trajectories\n",
    "    - Dyna-Q style: combine model learning with policy learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transition model: predicts next state\n",
    "        self.transition_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "        \n",
    "        # Reward model: predicts immediate reward\n",
    "        self.reward_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Done prediction\n",
    "        self.done_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action_onehot):\n",
    "        \"\"\"Predict next state, reward, and done flag\"\"\"\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        \n",
    "        next_state = self.transition_net(x)\n",
    "        reward = self.reward_net(x)\n",
    "        done_prob = self.done_net(x)\n",
    "        \n",
    "        return next_state, reward, done_prob\n",
    "    \n",
    "    def imagine_trajectory(self, initial_state, policy_fn, horizon: int = 10):\n",
    "        \"\"\"\n",
    "        Imagine a trajectory using the world model.\n",
    "        Used for planning or auxiliary training.\n",
    "        \"\"\"\n",
    "        states = [initial_state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        state = initial_state\n",
    "        for _ in range(horizon):\n",
    "            # Get action from policy\n",
    "            action = policy_fn(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Predict next state\n",
    "            action_onehot = F.one_hot(action, num_classes=9).float()\n",
    "            next_state, reward, done_prob = self.forward(state, action_onehot)\n",
    "            \n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # Stop if done is likely\n",
    "            if done_prob > 0.5:\n",
    "                break\n",
    "        \n",
    "        return states, actions, rewards\n",
    "\n",
    "class WorldModelAgent:\n",
    "    \"\"\"Agent that uses world model for planning\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 1e-3):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.world_model = WorldModel(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.world_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        print(f\"âœ“ World Model Agent initialized\")\n",
    "    \n",
    "    def train_world_model(self, batch_size: int = 64):\n",
    "        \"\"\"Train world model on collected experience\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = [self.replay_buffer[i] for i in np.random.choice(len(self.replay_buffer), batch_size)]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states_t = torch.FloatTensor(states).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        rewards_t = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states_t = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones_t = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # One-hot encode actions\n",
    "        actions_onehot = F.one_hot(actions_t, num_classes=9).float()\n",
    "        \n",
    "        # Predict\n",
    "        pred_next_state, pred_reward, pred_done = self.world_model(states_t, actions_onehot)\n",
    "        \n",
    "        # Losses\n",
    "        state_loss = F.mse_loss(pred_next_state, next_states_t)\n",
    "        reward_loss = F.mse_loss(pred_reward, rewards_t)\n",
    "        done_loss = F.binary_cross_entropy(pred_done, dones_t)\n",
    "        \n",
    "        total_loss = state_loss + reward_loss + done_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CURIOSITY MODULE (Intrinsic Motivation)\n",
    "# ============================================================================\n",
    "\n",
    "class CuriosityModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Intrinsic Curiosity Module (ICM) for exploration.\n",
    "    \n",
    "    RL Concept: Intrinsic motivation\n",
    "    - Reward agents for exploring novel states\n",
    "    - Prediction error as curiosity signal\n",
    "    - Helps with sparse reward environments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoder (for state representation)\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64)\n",
    "        )\n",
    "        \n",
    "        # Forward model: predict next state features from current + action\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(64 + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64)\n",
    "        )\n",
    "        \n",
    "        # Inverse model: predict action from state features\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(64 + 64, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, next_state, action):\n",
    "        \"\"\"Compute intrinsic reward based on prediction error\"\"\"\n",
    "        # Encode states\n",
    "        state_features = self.feature_encoder(state)\n",
    "        next_state_features = self.feature_encoder(next_state)\n",
    "        \n",
    "        # Forward model prediction\n",
    "        action_onehot = F.one_hot(action, num_classes=9).float()\n",
    "        predicted_next_features = self.forward_model(\n",
    "            torch.cat([state_features, action_onehot], dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Intrinsic reward = prediction error\n",
    "        intrinsic_reward = F.mse_loss(\n",
    "            predicted_next_features, next_state_features, reduction='none'\n",
    "        ).mean(dim=-1)\n",
    "        \n",
    "        # Inverse model prediction (for training)\n",
    "        predicted_action = self.inverse_model(\n",
    "            torch.cat([state_features, next_state_features], dim=-1)\n",
    "        )\n",
    "        \n",
    "        return intrinsic_reward, predicted_action\n",
    "\n",
    "class CuriosityAgent:\n",
    "    \"\"\"Agent with curiosity-driven exploration\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 base_agent, curiosity_weight: float = 0.5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.curiosity = CuriosityModule(state_dim, action_dim).to(self.device)\n",
    "        self.curiosity_optimizer = optim.Adam(self.curiosity.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.base_agent = base_agent\n",
    "        self.curiosity_weight = curiosity_weight\n",
    "        \n",
    "        print(f\"âœ“ Curiosity Agent initialized (weight={curiosity_weight})\")\n",
    "    \n",
    "    def compute_intrinsic_reward(self, state, next_state, action):\n",
    "        \"\"\"Add curiosity bonus to extrinsic reward\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "        action_t = torch.LongTensor([action]).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            intrinsic_reward, _ = self.curiosity(state_t, next_state_t, action_t)\n",
    "        \n",
    "        return intrinsic_reward.item()\n",
    "    \n",
    "    def train_curiosity(self, state, next_state, action):\n",
    "        \"\"\"Train curiosity module\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "        action_t = torch.LongTensor([action]).to(self.device)\n",
    "        \n",
    "        intrinsic_reward, predicted_action = self.curiosity(state_t, next_state_t, action_t)\n",
    "        \n",
    "        # Loss: forward model + inverse model\n",
    "        forward_loss = intrinsic_reward.mean()\n",
    "        inverse_loss = F.cross_entropy(predicted_action, action_t)\n",
    "        \n",
    "        loss = forward_loss + inverse_loss\n",
    "        \n",
    "        self.curiosity_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.curiosity_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. HIERARCHICAL RL (Options Framework)\n",
    "# ============================================================================\n",
    "\n",
    "class Option(nn.Module):\n",
    "    \"\"\"\n",
    "    A single option (sub-policy) in hierarchical RL.\n",
    "    \n",
    "    RL Concept: Temporal abstraction\n",
    "    - Options = skills that execute over multiple timesteps\n",
    "    - Learn when to initiate and terminate options\n",
    "    - Enables hierarchical decision making\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Intra-option policy: what actions to take within this option\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Termination function: when to end this option\n",
    "        self.termination = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Get action and termination probability\"\"\"\n",
    "        logits = self.policy(state)\n",
    "        termination_prob = self.termination(state)\n",
    "        return logits, termination_prob\n",
    "\n",
    "class HierarchicalAgent:\n",
    "    \"\"\"\n",
    "    Agent with hierarchical policy (meta-controller + options).\n",
    "    \n",
    "    Two-level hierarchy:\n",
    "    - Meta-controller: selects which option to use\n",
    "    - Options: execute low-level actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, n_options: int = 4):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_options = n_options\n",
    "        \n",
    "        # Options (sub-policies)\n",
    "        self.options = nn.ModuleList([\n",
    "            Option(state_dim, action_dim) for _ in range(n_options)\n",
    "        ]).to(self.device)\n",
    "        \n",
    "        # Meta-controller: selects option\n",
    "        self.meta_controller = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_options)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.options.parameters()) + list(self.meta_controller.parameters()),\n",
    "            lr=1e-4\n",
    "        )\n",
    "        \n",
    "        # Current option\n",
    "        self.current_option = None\n",
    "        self.option_duration = 0\n",
    "        \n",
    "        print(f\"âœ“ Hierarchical Agent initialized ({n_options} options)\")\n",
    "    \n",
    "    def select_action(self, state, force_new_option=False):\n",
    "        \"\"\"Select action using hierarchical policy\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Check if we need to select a new option\n",
    "        if self.current_option is None or force_new_option:\n",
    "            # Meta-controller selects option\n",
    "            with torch.no_grad():\n",
    "                option_logits = self.meta_controller(state_t)\n",
    "                option_probs = torch.softmax(option_logits, dim=-1)\n",
    "                self.current_option = torch.multinomial(option_probs, 1).item()\n",
    "                self.option_duration = 0\n",
    "        \n",
    "        # Execute current option\n",
    "        with torch.no_grad():\n",
    "            action_logits, termination_prob = self.options[self.current_option](state_t)\n",
    "            action_probs = torch.softmax(action_logits, dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        self.option_duration += 1\n",
    "        \n",
    "        # Check termination\n",
    "        if termination_prob.item() > 0.5 or self.option_duration > 20:\n",
    "            self.current_option = None\n",
    "        \n",
    "        return action, self.current_option\n",
    "\n",
    "# ============================================================================\n",
    "# 4. META-RL (Learning to Learn)\n",
    "# ============================================================================\n",
    "\n",
    "class MetaLearner:\n",
    "    \"\"\"\n",
    "    Meta-RL using MAML-style adaptation.\n",
    "    \n",
    "    RL Concept: Meta-learning\n",
    "    - Learn initialization that adapts quickly to new tasks\n",
    "    - Few-shot learning in RL\n",
    "    - Test on distribution of environments (seasons)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_network, inner_lr: float = 0.01, meta_lr: float = 1e-3):\n",
    "        self.base_network = base_network\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_optimizer = optim.Adam(base_network.parameters(), lr=meta_lr)\n",
    "        \n",
    "        print(f\"âœ“ Meta-Learner initialized (inner_lr={inner_lr}, meta_lr={meta_lr})\")\n",
    "    \n",
    "    def inner_loop_update(self, network, task_data, n_steps: int = 5):\n",
    "        \"\"\"\n",
    "        Inner loop: adapt to specific task.\n",
    "        Fast adaptation using gradient descent.\n",
    "        \"\"\"\n",
    "        adapted_network = copy.deepcopy(network)\n",
    "        inner_optimizer = optim.SGD(adapted_network.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            # Sample from task\n",
    "            states, actions, rewards, next_states, dones = task_data\n",
    "            \n",
    "            # Compute loss (simplified)\n",
    "            loss = self._compute_task_loss(adapted_network, states, actions, rewards)\n",
    "            \n",
    "            inner_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        \n",
    "        return adapted_network\n",
    "    \n",
    "    def meta_update(self, task_batch):\n",
    "        \"\"\"\n",
    "        Outer loop: update meta-parameters.\n",
    "        Meta-gradient across multiple tasks.\n",
    "        \"\"\"\n",
    "        meta_loss = 0\n",
    "        \n",
    "        for task_data in task_batch:\n",
    "            # Inner loop adaptation\n",
    "            adapted_network = self.inner_loop_update(self.base_network, task_data)\n",
    "            \n",
    "            # Evaluate on task\n",
    "            states, actions, rewards, next_states, dones = task_data\n",
    "            task_loss = self._compute_task_loss(adapted_network, states, actions, rewards)\n",
    "            \n",
    "            meta_loss += task_loss\n",
    "        \n",
    "        # Meta-optimization\n",
    "        meta_loss = meta_loss / len(task_batch)\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return meta_loss.item()\n",
    "    \n",
    "    def _compute_task_loss(self, network, states, actions, rewards):\n",
    "        \"\"\"Compute loss for a task (placeholder)\"\"\"\n",
    "        # This would be your actual RL loss (policy gradient, Q-learning, etc.)\n",
    "        # Simplified for demonstration\n",
    "        return torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CURRICULUM LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "class CurriculumManager:\n",
    "    \"\"\"\n",
    "    Manages curriculum for progressive training.\n",
    "    \n",
    "    RL Concept: Curriculum learning\n",
    "    - Start with easy tasks, gradually increase difficulty\n",
    "    - Automatic difficulty adjustment based on performance\n",
    "    - Accelerates learning on complex tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_difficulty: float = 0.1):\n",
    "        self.difficulty = initial_difficulty\n",
    "        self.performance_history = deque(maxlen=50)\n",
    "        \n",
    "        print(f\"âœ“ Curriculum Manager initialized (difficulty={initial_difficulty})\")\n",
    "    \n",
    "    def get_environment_config(self):\n",
    "        \"\"\"Generate environment config based on current difficulty\"\"\"\n",
    "        config = {\n",
    "            'grid_size': int(10 + self.difficulty * 30),  # 10 to 40\n",
    "            'n_agents': int(2 + self.difficulty * 6),      # 2 to 8\n",
    "            'food_spawn_rate': 0.03 * (1 - self.difficulty * 0.5),  # Decrease spawn rate\n",
    "            'obstacle_density': 0.05 + self.difficulty * 0.15,       # More obstacles\n",
    "            'max_steps': int(200 + self.difficulty * 300)            # Longer episodes\n",
    "        }\n",
    "        return config\n",
    "    \n",
    "    def update_difficulty(self, episode_reward: float, success: bool):\n",
    "        \"\"\"Adjust difficulty based on performance\"\"\"\n",
    "        self.performance_history.append((episode_reward, success))\n",
    "        \n",
    "        if len(self.performance_history) < 20:\n",
    "            return\n",
    "        \n",
    "        # Compute recent performance\n",
    "        recent_success_rate = sum(s for _, s in list(self.performance_history)[-20:]) / 20\n",
    "        recent_avg_reward = np.mean([r for r, _ in list(self.performance_history)[-20:]])\n",
    "        \n",
    "        # Increase difficulty if performing well\n",
    "        if recent_success_rate > 0.7 and self.difficulty < 1.0:\n",
    "            self.difficulty = min(1.0, self.difficulty + 0.05)\n",
    "            print(f\"ðŸ“ˆ Difficulty increased to {self.difficulty:.2f}\")\n",
    "        \n",
    "        # Decrease if struggling\n",
    "        elif recent_success_rate < 0.3 and self.difficulty > 0.1:\n",
    "            self.difficulty = max(0.1, self.difficulty - 0.05)\n",
    "            print(f\"ðŸ“‰ Difficulty decreased to {self.difficulty:.2f}\")\n",
    "    \n",
    "    def get_difficulty(self):\n",
    "        return self.difficulty\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OFFLINE RL / IMITATION LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "class OfflineRLAgent:\n",
    "    \"\"\"\n",
    "    Learn from fixed dataset (offline RL).\n",
    "    \n",
    "    RL Concepts:\n",
    "    - Batch RL: learn without environment interaction\n",
    "    - Behavioral cloning: imitate expert demonstrations\n",
    "    - Conservative Q-Learning: avoid OOD actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)\n",
    "        \n",
    "        print(f\"âœ“ Offline RL Agent initialized\")\n",
    "    \n",
    "    def behavioral_cloning(self, expert_states, expert_actions, n_epochs: int = 100):\n",
    "        \"\"\"\n",
    "        Learn to imitate expert policy.\n",
    "        Supervised learning on state-action pairs.\n",
    "        \"\"\"\n",
    "        states_t = torch.FloatTensor(expert_states).to(self.device)\n",
    "        actions_t = torch.LongTensor(expert_actions).to(self.device)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        print(f\"Training behavioral cloning on {len(expert_states)} demonstrations...\")\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Forward pass\n",
    "            logits = self.policy(states_t)\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, actions_t)\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using learned policy\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.policy(state_t)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "\n",
    "# ============================================================================\n",
    "# 7. INTEGRATED TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def train_with_curriculum(env_class, agent, curriculum_manager, n_iterations: int = 100):\n",
    "    \"\"\"Train agent with curriculum learning\"\"\"\n",
    "    rewards_history = []\n",
    "    difficulty_history = []\n",
    "    \n",
    "    print(\"\\nðŸŽ“ Training with Curriculum Learning...\")\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Get current difficulty config\n",
    "        config_dict = curriculum_manager.get_environment_config()\n",
    "        \n",
    "        # Create environment (you'd need to adapt your env to accept dict config)\n",
    "        # env = env_class(config_dict)\n",
    "        # For demo, just track difficulty\n",
    "        \n",
    "        # Train for some episodes at this difficulty\n",
    "        # episode_reward, success = train_episode(env, agent)\n",
    "        episode_reward = np.random.randn() * 10 + 50  # Placeholder\n",
    "        success = episode_reward > 40\n",
    "        \n",
    "        # Update curriculum\n",
    "        curriculum_manager.update_difficulty(episode_reward, success)\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        difficulty_history.append(curriculum_manager.get_difficulty())\n",
    "        \n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            print(f\"Iteration {iteration+1}: Reward={episode_reward:.1f}, Difficulty={curriculum_manager.get_difficulty():.2f}\")\n",
    "    \n",
    "    return rewards_history, difficulty_history\n",
    "\n",
    "def collect_expert_demonstrations(env, expert_agent, n_episodes: int = 50):\n",
    "    \"\"\"Collect demonstrations from expert for offline learning\"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    print(f\"Collecting {n_episodes} expert demonstrations...\")\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Expert action (assumes single agent)\n",
    "            action = expert_agent.select_action(obs[0])\n",
    "            states.append(obs[0]['state'])  # Simplified: just use state vector\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Step\n",
    "            next_obs, rewards, dones, truncated, _ = env.step([action] * env.config.n_agents)\n",
    "            obs = next_obs\n",
    "            done = dones[0] or truncated[0]\n",
    "    \n",
    "    print(f\"âœ“ Collected {len(states)} state-action pairs\")\n",
    "    return np.array(states), np.array(actions)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_advanced_metrics(results_dict):\n",
    "    \"\"\"Plot results from advanced RL experiments\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # World model loss\n",
    "    if 'world_model_loss' in results_dict:\n",
    "        axes[0, 0].plot(results_dict['world_model_loss'])\n",
    "        axes[0, 0].set_title('World Model Training Loss')\n",
    "        axes[0, 0].set_xlabel('Update Step')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Intrinsic rewards\n",
    "    if 'intrinsic_rewards' in results_dict:\n",
    "        axes[0, 1].plot(results_dict['intrinsic_rewards'], alpha=0.5)\n",
    "        axes[0, 1].set_title('Curiosity: Intrinsic Rewards')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Intrinsic Reward')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Curriculum difficulty\n",
    "    if 'curriculum_difficulty' in results_dict:\n",
    "        axes[1, 0].plot(results_dict['curriculum_difficulty'], linewidth=2)\n",
    "        axes[1, 0].set_title('Curriculum Learning Progress')\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Difficulty')\n",
    "        axes[1, 0].set_ylim([0, 1.1])\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Option usage (hierarchical)\n",
    "    if 'option_usage' in results_dict:\n",
    "        option_counts = results_dict['option_usage']\n",
    "        axes[1, 1].bar(range(len(option_counts)), option_counts)\n",
    "        axes[1, 1].set_title('Hierarchical RL: Option Usage')\n",
    "        axes[1, 1].set_xlabel('Option ID')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('Advanced RL Metrics', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO & EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED RL MODULES READY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ§  Implemented Concepts:\")\n",
    "print(\"  âœ“ World Models (model-based planning)\")\n",
    "print(\"  âœ“ Curiosity-driven exploration (ICM)\")\n",
    "print(\"  âœ“ Hierarchical RL (Options framework)\")\n",
    "print(\"  âœ“ Meta-learning (MAML-style)\")\n",
    "print(\"  âœ“ Curriculum learning\")\n",
    "print(\"  âœ“ Offline RL / Behavioral cloning\")\n",
    "print(\"\\nðŸ”¬ Experiment Ideas:\")\n",
    "print(\"  1. Compare model-free vs model-based agents\")\n",
    "print(\"  2. Measure exploration with/without curiosity\")\n",
    "print(\"  3. Emergent skills in hierarchical agents\")\n",
    "print(\"  4. Fast adaptation with meta-learning\")\n",
    "print(\"  5. Learning curves with curriculum\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# 1. World Model\n",
    "state_dim = 5\n",
    "action_dim = 9\n",
    "wm_agent = WorldModelAgent(state_dim, action_dim)\n",
    "\n",
    "# 2. Curiosity\n",
    "from part2_agents import PPOAgent  # Your base agent\n",
    "base_ppo = PPOAgent(...)\n",
    "curious_agent = CuriosityAgent(state_dim, action_dim, base_ppo)\n",
    "\n",
    "# 3. Hierarchical RL\n",
    "h_agent = HierarchicalAgent(state_dim, action_dim, n_options=4)\n",
    "\n",
    "# 4. Curriculum\n",
    "curriculum = CurriculumManager(initial_difficulty=0.1)\n",
    "# rewards, difficulties = train_with_curriculum(ColonyEnvironment, agent, curriculum)\n",
    "\n",
    "# 5. Offline RL / Imitation\n",
    "# expert_states, expert_actions = collect_expert_demonstrations(env, expert_agent)\n",
    "# offline_agent = OfflineRLAgent(state_dim, action_dim)\n",
    "# offline_agent.behavioral_cloning(expert_states, expert_actions)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

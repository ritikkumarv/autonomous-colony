{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a660bca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† The Autonomous Colony - Advanced RL\n",
    "\n",
    "## Part 4: Meta-Learning, World Models, Hierarchical RL, and Curiosity\n",
    "\n",
    "### RL Concepts Covered:\n",
    "1. **Meta-RL** / Learning to Learn (MAML-style)\n",
    "2. **World Models** (simplified DreamerV3)\n",
    "3. **Hierarchical RL** (Options Framework)\n",
    "4. **Intrinsic Motivation** (Curiosity/ICM)\n",
    "5. **Offline RL** / Imitation Learning\n",
    "6. **Curriculum Learning**\n",
    "7. **Model-based planning**\n",
    "\n",
    "### Prerequisites:\n",
    "- Parts 1-3 (environment, agents, multi-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858d85b",
   "metadata": {},
   "source": [
    "## ‚úÖ Advanced RL Techniques Ready!\n",
    "\n",
    "**Key Concepts Implemented:**\n",
    "- ‚úì World Models for model-based planning\n",
    "- ‚úì Curiosity-driven exploration (ICM)\n",
    "- ‚úì Hierarchical RL with Options framework\n",
    "- ‚úì Meta-learning for fast adaptation\n",
    "- ‚úì Curriculum learning for efficient training\n",
    "\n",
    "**These advanced techniques can significantly improve:**\n",
    "- Sample efficiency\n",
    "- Exploration in sparse reward environments\n",
    "- Transfer learning across tasks\n",
    "- Long-horizon planning and decision making\n",
    "\n",
    "**Combine these with agents from Parts 2-3 for state-of-the-art performance!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e414f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumScheduler:\n",
    "    \"\"\"\n",
    "    Automatically adjust environment difficulty based on agent performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_difficulty=0.3, success_threshold=0.7, window_size=20):\n",
    "        self.difficulty = initial_difficulty  # 0.0 to 1.0\n",
    "        self.success_threshold = success_threshold\n",
    "        self.window_size = window_size\n",
    "        self.recent_results = deque(maxlen=window_size)\n",
    "        \n",
    "        print(f\"‚úì Curriculum Scheduler initialized (difficulty={initial_difficulty})\")\n",
    "    \n",
    "    def record_episode(self, success: bool):\n",
    "        \"\"\"Record episode outcome\"\"\"\n",
    "        self.recent_results.append(1.0 if success else 0.0)\n",
    "    \n",
    "    def update_difficulty(self):\n",
    "        \"\"\"Adjust difficulty based on recent performance\"\"\"\n",
    "        if len(self.recent_results) < self.window_size:\n",
    "            return self.difficulty\n",
    "        \n",
    "        success_rate = np.mean(self.recent_results)\n",
    "        \n",
    "        if success_rate > self.success_threshold + 0.1:\n",
    "            # Too easy, increase difficulty\n",
    "            self.difficulty = min(1.0, self.difficulty + 0.05)\n",
    "            print(f\"üìà Increasing difficulty to {self.difficulty:.2f} (success rate: {success_rate:.2%})\")\n",
    "        elif success_rate < self.success_threshold - 0.1:\n",
    "            # Too hard, decrease difficulty\n",
    "            self.difficulty = max(0.1, self.difficulty - 0.05)\n",
    "            print(f\"üìâ Decreasing difficulty to {self.difficulty:.2f} (success rate: {success_rate:.2%})\")\n",
    "        \n",
    "        return self.difficulty\n",
    "    \n",
    "    def get_env_config(self, base_config):\n",
    "        \"\"\"Generate environment config based on current difficulty\"\"\"\n",
    "        config = copy.copy(base_config)\n",
    "        \n",
    "        # Adjust parameters based on difficulty\n",
    "        config.obstacle_density = 0.05 + (0.15 * self.difficulty)\n",
    "        config.food_spawn_rate = 0.03 - (0.015 * self.difficulty)\n",
    "        config.energy_decay = 0.05 + (0.15 * self.difficulty)\n",
    "        \n",
    "        return config\n",
    "\n",
    "print(\"‚úì Curriculum Learning scheduler defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee968221",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Curriculum Learning\n",
    "\n",
    "Progressively increase task difficulty for more efficient learning.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Automatic difficulty adjustment\n",
    "- Success-based progression\n",
    "- Staged learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95367ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner:\n",
    "    \"\"\"\n",
    "    Simplified meta-learning for fast adaptation.\n",
    "    Based on MAML principles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, meta_lr=1e-3, inner_lr=1e-2):\n",
    "        self.model = model\n",
    "        self.meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        \n",
    "        print(\"‚úì Meta-Learner initialized\")\n",
    "    \n",
    "    def inner_loop(self, task_data, n_steps=5):\n",
    "        \"\"\"Adapt to specific task (inner loop)\"\"\"\n",
    "        # Clone model for task-specific adaptation\n",
    "        adapted_model = copy.deepcopy(self.model)\n",
    "        optimizer = optim.SGD(adapted_model.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            # Task-specific training step\n",
    "            loss = self.compute_task_loss(adapted_model, task_data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return adapted_model\n",
    "    \n",
    "    def meta_update(self, task_batch):\n",
    "        \"\"\"Meta-learning outer loop across tasks\"\"\"\n",
    "        meta_loss = 0\n",
    "        \n",
    "        for task_data in task_batch:\n",
    "            # Adapt to task\n",
    "            adapted_model = self.inner_loop(task_data['train'])\n",
    "            \n",
    "            # Evaluate on task test set\n",
    "            task_loss = self.compute_task_loss(adapted_model, task_data['test'])\n",
    "            meta_loss += task_loss\n",
    "        \n",
    "        # Update meta-model\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return meta_loss.item()\n",
    "    \n",
    "    def compute_task_loss(self, model, data):\n",
    "        \"\"\"Placeholder for task-specific loss\"\"\"\n",
    "        # Implementation depends on specific task\n",
    "        return torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "print(\"‚úì Meta-Learning framework defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30158ffa",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Meta-Learning (Learning to Learn)\n",
    "\n",
    "Simplified meta-learning approach for fast adaptation to new tasks.\n",
    "\n",
    "**Key Concepts:**\n",
    "- MAML (Model-Agnostic Meta-Learning)\n",
    "- Few-shot adaptation\n",
    "- Task distribution learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option:\n",
    "    \"\"\"A single option (skill) with initiation, policy, and termination\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, policy_fn, termination_fn, initiation_fn=None):\n",
    "        self.name = name\n",
    "        self.policy = policy_fn\n",
    "        self.termination = termination_fn\n",
    "        self.initiation = initiation_fn or (lambda s: True)\n",
    "    \n",
    "    def can_initiate(self, state):\n",
    "        \"\"\"Check if option can be initiated in this state\"\"\"\n",
    "        return self.initiation(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get primitive action from option's policy\"\"\"\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def should_terminate(self, state):\n",
    "        \"\"\"Check if option should terminate\"\"\"\n",
    "        return self.termination(state)\n",
    "\n",
    "class HierarchicalAgent:\n",
    "    \"\"\"Agent that learns and executes options\"\"\"\n",
    "    \n",
    "    def __init__(self, options: List[Option]):\n",
    "        self.options = options\n",
    "        self.current_option = None\n",
    "        self.option_history = []\n",
    "        \n",
    "        print(f\"‚úì Hierarchical Agent with {len(options)} options\")\n",
    "    \n",
    "    def select_option(self, state):\n",
    "        \"\"\"Select which option to execute (meta-policy)\"\"\"\n",
    "        # Simple: random among available options\n",
    "        available = [opt for opt in self.options if opt.can_initiate(state)]\n",
    "        if available:\n",
    "            return np.random.choice(available)\n",
    "        return None\n",
    "    \n",
    "    def step(self, state):\n",
    "        \"\"\"Execute one step with hierarchical control\"\"\"\n",
    "        # If no current option or option terminated, select new option\n",
    "        if self.current_option is None or self.current_option.should_terminate(state):\n",
    "            self.current_option = self.select_option(state)\n",
    "            if self.current_option:\n",
    "                self.option_history.append(self.current_option.name)\n",
    "        \n",
    "        # Get action from current option\n",
    "        if self.current_option:\n",
    "            return self.current_option.get_action(state)\n",
    "        else:\n",
    "            return 0  # Default action\n",
    "\n",
    "# Example options\n",
    "def explore_policy(state):\n",
    "    return np.random.randint(0, 8)  # Random movement\n",
    "\n",
    "def collect_policy(state):\n",
    "    return 8  # Collect action\n",
    "\n",
    "def explore_terminate(state):\n",
    "    # Terminate if see resource nearby\n",
    "    grid = state['grid']\n",
    "    center = grid.shape[0] // 2\n",
    "    local = grid[center-1:center+2, center-1:center+2, 1:3]  # Food/water channels\n",
    "    return local.sum() > 0\n",
    "\n",
    "def collect_terminate(state):\n",
    "    # Terminate after collecting\n",
    "    return True\n",
    "\n",
    "explore_option = Option(\"explore\", explore_policy, explore_terminate)\n",
    "collect_option = Option(\"collect\", collect_policy, collect_terminate)\n",
    "\n",
    "print(\"‚úì Options Framework defined with example options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74f5d7",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Hierarchical RL (Options Framework)\n",
    "\n",
    "Options framework for temporal abstraction and hierarchical policies.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Temporal abstraction\n",
    "- Options/Skills\n",
    "- Hierarchical decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuriosityModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Intrinsic Curiosity Module (ICM) for exploration.\n",
    "    Provides intrinsic reward based on prediction error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, feature_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoder\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Inverse model: predict action from state transition\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Forward model: predict next state features from current state and action\n",
    "        self.forward_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, feature_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, next_state, action):\n",
    "        \"\"\"Compute intrinsic reward (prediction error)\"\"\"\n",
    "        # Encode states\n",
    "        state_feat = self.feature_net(state)\n",
    "        next_state_feat = self.feature_net(next_state)\n",
    "        \n",
    "        # Forward model prediction\n",
    "        action_onehot = F.one_hot(action, num_classes=9).float()\n",
    "        predicted_next_feat = self.forward_net(torch.cat([state_feat, action_onehot], dim=-1))\n",
    "        \n",
    "        # Intrinsic reward = prediction error\n",
    "        intrinsic_reward = F.mse_loss(predicted_next_feat, next_state_feat, reduction='none').mean(dim=-1)\n",
    "        \n",
    "        # Inverse model loss (for learning)\n",
    "        predicted_action_logits = self.inverse_net(torch.cat([state_feat, next_state_feat], dim=-1))\n",
    "        \n",
    "        return intrinsic_reward, predicted_action_logits\n",
    "\n",
    "print(\"‚úì Curiosity Module (ICM) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011325b",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Curiosity-Driven Exploration (ICM)\n",
    "\n",
    "Intrinsic Curiosity Module for exploration bonus based on prediction error.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Intrinsic motivation\n",
    "- Forward/inverse dynamics models\n",
    "- Exploration bonuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified world model for planning.\n",
    "    Learns to predict next state and reward given current state and action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transition model: predicts next state\n",
    "        self.transition_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "        \n",
    "        # Reward model: predicts immediate reward\n",
    "        self.reward_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Done prediction\n",
    "        self.done_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action_onehot):\n",
    "        \"\"\"Predict next state, reward, and done flag\"\"\"\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        \n",
    "        next_state = self.transition_net(x)\n",
    "        reward = self.reward_net(x)\n",
    "        done_prob = self.done_net(x)\n",
    "        \n",
    "        return next_state, reward, done_prob\n",
    "    \n",
    "    def imagine_trajectory(self, initial_state, policy_fn, horizon: int = 10):\n",
    "        \"\"\"Imagine a trajectory using the world model for planning\"\"\"\n",
    "        states = [initial_state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        state = initial_state\n",
    "        for _ in range(horizon):\n",
    "            action = policy_fn(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            action_onehot = F.one_hot(action, num_classes=9).float()\n",
    "            next_state, reward, done_prob = self.forward(state, action_onehot)\n",
    "            \n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done_prob > 0.5:\n",
    "                break\n",
    "        \n",
    "        return states, actions, rewards\n",
    "\n",
    "print(\"‚úì World Model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396baad",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ World Model (Model-Based RL)\n",
    "\n",
    "Learn a dynamics model to predict next states and rewards, enabling planning.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Model-based RL: learn environment dynamics\n",
    "- Planning: use model to simulate trajectories\n",
    "- Dyna-Q style: combine model learning with policy learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03782a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch numpy matplotlib -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "print(\"‚úì Advanced RL modules loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45859d0c",
   "metadata": {},
   "source": [
    "## üì¶ Setup - Install Dependencies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

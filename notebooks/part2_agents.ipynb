{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a021e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# THE AUTONOMOUS COLONY - PART 2: RL AGENTS\n",
    "# From Tabular Q-Learning to Deep RL (DQN, PPO, SAC)\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "# ðŸ¤– The Autonomous Colony - RL Agents\n",
    "\n",
    "This notebook implements multiple RL algorithms for the colony environment.\n",
    "\n",
    "**RL Concepts Covered:**\n",
    "1. Tabular Q-Learning (value-based, discrete)\n",
    "2. DQN with experience replay (deep Q-learning)\n",
    "3. PPO (policy gradient, actor-critic)\n",
    "4. Exploration strategies (Îµ-greedy, entropy bonus)\n",
    "5. Reward shaping and sparse rewards\n",
    "6. Training curves and evaluation\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run Part 1 first to set up the environment\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# !pip install gymnasium stable-baselines3 torch tensorboard -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "import copy\n",
    "\n",
    "# Import environment from Part 1 (assumes it's in the same notebook or imported)\n",
    "# from part1_environment import ColonyEnvironment, EnvironmentConfig, ActionType\n",
    "\n",
    "print(\"âœ“ Dependencies loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TABULAR Q-LEARNING AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class TabularQLearningAgent:\n",
    "    \"\"\"\n",
    "    Classic Q-Learning with table lookup.\n",
    "    \n",
    "    RL Concepts:\n",
    "    - Value-based learning\n",
    "    - Temporal Difference (TD) learning\n",
    "    - Îµ-greedy exploration\n",
    "    - Bellman equation: Q(s,a) = Q(s,a) + Î±[r + Î³Â·max_a'Q(s',a') - Q(s,a)]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q-table: state -> action values\n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_dim))\n",
    "        self.state_visits = defaultdict(int)\n",
    "        \n",
    "    def _discretize_state(self, observation: Dict) -> tuple:\n",
    "        \"\"\"Convert continuous observation to discrete state\"\"\"\n",
    "        state_vec = observation['state']\n",
    "        # Discretize to 10 bins\n",
    "        discrete = tuple((state_vec * 10).astype(int).clip(0, 9))\n",
    "        \n",
    "        # Add grid info (simplified)\n",
    "        grid = observation['grid']\n",
    "        # Count nearby resources\n",
    "        center = grid.shape[0] // 2\n",
    "        local_view = grid[center-1:center+2, center-1:center+2, :]\n",
    "        food_count = int(local_view[:, :, 1].sum())\n",
    "        water_count = int(local_view[:, :, 2].sum())\n",
    "        \n",
    "        return discrete + (food_count, water_count)\n",
    "    \n",
    "    def select_action(self, observation: Dict, training: bool = True) -> int:\n",
    "        \"\"\"Îµ-greedy action selection\"\"\"\n",
    "        state = self._discretize_state(observation)\n",
    "        \n",
    "        # Exploration\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        # Exploitation\n",
    "        q_values = self.q_table[state]\n",
    "        return int(np.argmax(q_values))\n",
    "    \n",
    "    def update(self, state: Dict, action: int, reward: float, \n",
    "               next_state: Dict, done: bool):\n",
    "        \"\"\"Q-Learning update\"\"\"\n",
    "        s = self._discretize_state(state)\n",
    "        s_next = self._discretize_state(next_state)\n",
    "        \n",
    "        # Current Q-value\n",
    "        q_current = self.q_table[s][action]\n",
    "        \n",
    "        # Target Q-value\n",
    "        if done:\n",
    "            q_target = reward\n",
    "        else:\n",
    "            q_target = reward + self.gamma * np.max(self.q_table[s_next])\n",
    "        \n",
    "        # TD error\n",
    "        td_error = q_target - q_current\n",
    "        \n",
    "        # Update\n",
    "        self.q_table[s][action] += self.lr * td_error\n",
    "        self.state_visits[s] += 1\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DEEP Q-NETWORK (DQN)\n",
    "# ============================================================================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for Q-value approximation.\n",
    "    Handles both grid observations and internal state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_shape: tuple, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for grid observation\n",
    "        self.conv1 = nn.Conv2d(grid_shape[2], 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Calculate conv output size\n",
    "        conv_out_size = grid_shape[0] * grid_shape[1] * 64\n",
    "        \n",
    "        # FC for internal state\n",
    "        self.state_fc = nn.Linear(state_dim, 64)\n",
    "        \n",
    "        # Combined layers\n",
    "        self.fc1 = nn.Linear(conv_out_size + 64, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        \n",
    "    def forward(self, grid_obs, state_obs):\n",
    "        # Process grid\n",
    "        x = F.relu(self.conv1(grid_obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        # Process state\n",
    "        s = F.relu(self.state_fc(state_obs))\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat([x, s], dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network with experience replay and target network.\n",
    "    \n",
    "    RL Concepts:\n",
    "    - Function approximation (neural networks)\n",
    "    - Experience replay (decorrelation)\n",
    "    - Target network (stability)\n",
    "    - Double DQN (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_shape: tuple,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        learning_rate: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01,\n",
    "        buffer_size: int = 10000,\n",
    "        batch_size: int = 64,\n",
    "        target_update_freq: int = 100\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(grid_shape, state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(grid_shape, state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training stats\n",
    "        self.update_count = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "        print(f\"âœ“ DQN Agent initialized on {self.device}\")\n",
    "    \n",
    "    def _obs_to_tensor(self, obs: Dict):\n",
    "        \"\"\"Convert observation dict to tensors\"\"\"\n",
    "        grid = torch.FloatTensor(obs['grid']).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "        state = torch.FloatTensor(obs['state']).unsqueeze(0).to(self.device)\n",
    "        return grid, state\n",
    "    \n",
    "    def select_action(self, observation: Dict, training: bool = True) -> int:\n",
    "        \"\"\"Îµ-greedy action selection\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        grid, state = self._obs_to_tensor(observation)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(grid, state)\n",
    "        return int(q_values.argmax().item())\n",
    "    \n",
    "    def update(self, state: Dict, action: int, reward: float,\n",
    "               next_state: Dict, done: bool):\n",
    "        \"\"\"Store transition and train if buffer is ready\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        grids = torch.stack([torch.FloatTensor(s['grid']).permute(2, 0, 1) for s in states]).to(self.device)\n",
    "        state_vecs = torch.stack([torch.FloatTensor(s['state']) for s in states]).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_grids = torch.stack([torch.FloatTensor(s['grid']).permute(2, 0, 1) for s in next_states]).to(self.device)\n",
    "        next_state_vecs = torch.stack([torch.FloatTensor(s['state']) for s in next_states]).to(self.device)\n",
    "        dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(grids, state_vecs).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_grids, next_state_vecs).max(1)[0]\n",
    "            target_q = rewards_t + self.gamma * next_q * (1 - dones_t)\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.loss_history.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PPO AGENT (Policy Gradient)\n",
    "# ============================================================================\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for PPO.\n",
    "    Actor outputs policy, Critic outputs value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_shape: tuple, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.conv1 = nn.Conv2d(grid_shape[2], 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        conv_out_size = grid_shape[0] * grid_shape[1] * 64\n",
    "        \n",
    "        self.state_fc = nn.Linear(state_dim, 64)\n",
    "        self.shared_fc = nn.Linear(conv_out_size + 64, 256)\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor_fc = nn.Linear(256, 128)\n",
    "        self.actor_out = nn.Linear(128, action_dim)\n",
    "        \n",
    "        # Critic head (value)\n",
    "        self.critic_fc = nn.Linear(256, 128)\n",
    "        self.critic_out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, grid_obs, state_obs):\n",
    "        # Shared features\n",
    "        x = F.relu(self.conv1(grid_obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        s = F.relu(self.state_fc(state_obs))\n",
    "        features = F.relu(self.shared_fc(torch.cat([x, s], dim=1)))\n",
    "        \n",
    "        # Actor (policy logits)\n",
    "        actor_x = F.relu(self.actor_fc(features))\n",
    "        logits = self.actor_out(actor_x)\n",
    "        \n",
    "        # Critic (value)\n",
    "        critic_x = F.relu(self.critic_fc(features))\n",
    "        value = self.critic_out(critic_x)\n",
    "        \n",
    "        return logits, value\n",
    "    \n",
    "    def get_action_and_value(self, grid_obs, state_obs, action=None):\n",
    "        logits, value = self(grid_obs, state_obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        \n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action, log_prob, entropy, value\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization - modern policy gradient method.\n",
    "    \n",
    "    RL Concepts:\n",
    "    - Policy gradient theorem\n",
    "    - Actor-Critic architecture\n",
    "    - Clipped surrogate objective\n",
    "    - Generalized Advantage Estimation (GAE)\n",
    "    - Entropy regularization for exploration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_shape: tuple,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        learning_rate: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        n_epochs: int = 4,\n",
    "        batch_size: int = 64\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.network = ActorCritic(grid_shape, state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Rollout storage\n",
    "        self.rollout_buffer = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        print(f\"âœ“ PPO Agent initialized on {self.device}\")\n",
    "    \n",
    "    def select_action(self, observation: Dict, training: bool = True):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        grid = torch.FloatTensor(observation['grid']).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "        state = torch.FloatTensor(observation['state']).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy, value = self.network.get_action_and_value(grid, state)\n",
    "        \n",
    "        return int(action.item()), log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, value, done):\n",
    "        \"\"\"Store transition for batch update\"\"\"\n",
    "        self.rollout_buffer.append((state, action, reward, log_prob, value, done))\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        returns = [adv + val for adv, val in zip(advantages, values)]\n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"PPO update using collected rollouts\"\"\"\n",
    "        if len(self.rollout_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Prepare batch\n",
    "        states, actions, rewards, old_log_probs, values, dones = zip(*self.rollout_buffer)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages, returns = self.compute_gae(list(rewards), list(values), list(dones))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        grids = torch.stack([torch.FloatTensor(s['grid']).permute(2, 0, 1) for s in states]).to(self.device)\n",
    "        state_vecs = torch.stack([torch.FloatTensor(s['state']) for s in states]).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        old_log_probs_t = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        advantages_t = torch.FloatTensor(advantages).to(self.device)\n",
    "        returns_t = torch.FloatTensor(returns).to(self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)\n",
    "        \n",
    "        # PPO epochs\n",
    "        total_loss = 0\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Get current policy\n",
    "            _, new_log_probs, entropy, values_pred = self.network.get_action_and_value(\n",
    "                grids, state_vecs, actions_t\n",
    "            )\n",
    "            \n",
    "            # Ratio for clipped objective\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs_t)\n",
    "            \n",
    "            # Policy loss with clipping\n",
    "            surr1 = ratio * advantages_t\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages_t\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values_pred.squeeze(), returns_t)\n",
    "            \n",
    "            # Entropy bonus\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / self.n_epochs\n",
    "        self.loss_history.append(avg_loss)\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.rollout_buffer = []\n",
    "        \n",
    "        return avg_loss\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def train_agent(env, agent, n_episodes: int = 100, agent_type: str = \"dqn\"):\n",
    "    \"\"\"\n",
    "    Universal training loop for different agent types.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"\\nðŸš€ Training {agent_type.upper()} agent for {n_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        observations, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        # For single agent (first agent only)\n",
    "        obs = observations[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done and episode_length < env.config.max_steps:\n",
    "            # Select action\n",
    "            if agent_type == \"ppo\":\n",
    "                action, log_prob, value = agent.select_action(obs)\n",
    "            else:\n",
    "                action = agent.select_action(obs)\n",
    "            \n",
    "            # Step environment (all agents take same action for simplicity)\n",
    "            actions = [action] * env.config.n_agents\n",
    "            next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "            \n",
    "            next_obs = next_observations[0]\n",
    "            reward = rewards[0]\n",
    "            done = dones[0] or truncated[0]\n",
    "            \n",
    "            # Update agent\n",
    "            if agent_type == \"ppo\":\n",
    "                agent.store_transition(obs, action, reward, log_prob, value, done)\n",
    "                if len(agent.rollout_buffer) >= agent.batch_size:\n",
    "                    loss = agent.update()\n",
    "                    if loss:\n",
    "                        episode_loss.append(loss)\n",
    "            else:\n",
    "                loss = agent.update(obs, action, reward, next_obs, done)\n",
    "                if loss:\n",
    "                    episode_loss.append(loss)\n",
    "            \n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        # Decay exploration\n",
    "        if hasattr(agent, 'decay_epsilon'):\n",
    "            agent.decay_epsilon()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            eps = agent.epsilon if hasattr(agent, 'epsilon') else 0\n",
    "            print(f\"Episode {episode+1}/{n_episodes} | Avg Reward: {avg_reward:.2f} | Avg Length: {avg_length:.1f} | Îµ: {eps:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'losses': losses\n",
    "    }\n",
    "\n",
    "def plot_training_results(results: Dict, title: str):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Rewards\n",
    "    rewards = results['rewards']\n",
    "    axes[0].plot(rewards, alpha=0.3, label='Raw')\n",
    "    if len(rewards) > 10:\n",
    "        smoothed = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "        axes[0].plot(range(9, len(rewards)), smoothed, linewidth=2, label='Smoothed (10)')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('Episode Rewards')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Lengths\n",
    "    lengths = results['lengths']\n",
    "    axes[1].plot(lengths, alpha=0.3, label='Raw')\n",
    "    if len(lengths) > 10:\n",
    "        smoothed = np.convolve(lengths, np.ones(10)/10, mode='valid')\n",
    "        axes[1].plot(range(9, len(lengths)), smoothed, linewidth=2, label='Smoothed (10)')\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Episode Length')\n",
    "    axes[1].set_title('Episode Lengths')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Losses\n",
    "    if results['losses']:\n",
    "        axes[2].plot(results['losses'], alpha=0.6)\n",
    "        axes[2].set_xlabel('Update Step')\n",
    "        axes[2].set_ylabel('Loss')\n",
    "        axes[2].set_title('Training Loss')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO: Train Different Agents\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY TO TRAIN RL AGENTS!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAvailable agents:\")\n",
    "print(\"1. TabularQLearningAgent - Classic Q-learning\")\n",
    "print(\"2. DQNAgent - Deep Q-Network\")\n",
    "print(\"3. PPOAgent - Proximal Policy Optimization\")\n",
    "print(\"\\nUncomment the training blocks below to run experiments!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

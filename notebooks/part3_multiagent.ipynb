{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9cb31a",
   "metadata": {},
   "source": [
    "# ü§ù The Autonomous Colony - Multi-Agent Coordination\n",
    "\n",
    "## Part 3: Cooperation, Competition, and Emergent Behaviors\n",
    "\n",
    "### RL Concepts Covered:\n",
    "1. **Multi-Agent RL (MARL)** fundamentals\n",
    "2. **Communication** between agents\n",
    "3. **Centralized training, decentralized execution** (CTDE)\n",
    "4. **Reward shaping** for cooperation\n",
    "5. **Nash equilibria** and social dilemmas\n",
    "6. **Parameter sharing** vs independent learners\n",
    "7. **PettingZoo** integration\n",
    "\n",
    "### Prerequisites:\n",
    "- Parts 1 & 2 (environment and single-agent RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned communication between agents.\n",
    "    Each agent can send/receive messages to nearby agents.\n",
    "    \n",
    "    RL Concept: Communication as part of the action space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, message_dim: int = 16, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.message_dim = message_dim\n",
    "        \n",
    "        # Message encoder (from agent state to message)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(5, hidden_dim),  # 5 = agent state features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, message_dim)\n",
    "        )\n",
    "        \n",
    "        # Message aggregator (combine received messages)\n",
    "        self.aggregator = nn.Sequential(\n",
    "            nn.Linear(message_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "    \n",
    "    def encode_message(self, agent_state):\n",
    "        \"\"\"Generate message from agent state\"\"\"\n",
    "        return self.encoder(agent_state)\n",
    "    \n",
    "    def aggregate_messages(self, messages):\n",
    "        \"\"\"Aggregate messages from multiple agents\"\"\"\n",
    "        if len(messages) == 0:\n",
    "            return torch.zeros(1, self.message_dim)\n",
    "        \n",
    "        # Mean pooling\n",
    "        combined = torch.stack(messages).mean(dim=0)\n",
    "        return self.aggregator(combined)\n",
    "\n",
    "print(\"‚úì Communication Network defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434b8f8",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Multi-Agent Actor-Critic\n",
    "\n",
    "**Key Features:**\n",
    "- Individual observations + messages ‚Üí features\n",
    "- Actor: decentralized policy per agent\n",
    "- Critic: can use global state (CTDE - Centralized Training, Decentralized Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic with communication for multi-agent settings.\n",
    "    \n",
    "    Architecture:\n",
    "    - Individual observations + messages -> features\n",
    "    - Actor: decentralized policy per agent\n",
    "    - Critic: can use global state (CTDE)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_shape: tuple,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        n_agents: int,\n",
    "        message_dim: int = 16,\n",
    "        use_communication: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.use_communication = use_communication\n",
    "        \n",
    "        # Communication module\n",
    "        if use_communication:\n",
    "            self.comm = CommunicationNetwork(message_dim)\n",
    "            extra_dim = message_dim\n",
    "        else:\n",
    "            extra_dim = 0\n",
    "        \n",
    "        # Individual feature extractors (decentralized)\n",
    "        self.conv1 = nn.Conv2d(grid_shape[2], 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        conv_out = grid_shape[0] * grid_shape[1] * 64\n",
    "        \n",
    "        self.state_fc = nn.Linear(state_dim, 64)\n",
    "        self.feature_fc = nn.Linear(conv_out + 64 + extra_dim, 256)\n",
    "        \n",
    "        # Actor (decentralized - per agent policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Critic (centralized - global value)\n",
    "        self.critic_fc = nn.Linear(256 * n_agents, 256)\n",
    "        self.critic_out = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward_actor(self, grid_obs, state_obs, messages=None):\n",
    "        \"\"\"Forward pass for actor (decentralized)\"\"\"\n",
    "        x = F.relu(self.conv1(grid_obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        s = F.relu(self.state_fc(state_obs))\n",
    "        \n",
    "        if self.use_communication and messages is not None:\n",
    "            features = torch.cat([x, s, messages], dim=1)\n",
    "        else:\n",
    "            features = torch.cat([x, s], dim=1)\n",
    "        \n",
    "        features = F.relu(self.feature_fc(features))\n",
    "        logits = self.actor(features)\n",
    "        \n",
    "        return logits, features\n",
    "    \n",
    "    def forward_critic(self, all_agent_features):\n",
    "        \"\"\"Forward pass for critic (centralized)\"\"\"\n",
    "        combined = torch.cat(all_agent_features, dim=1)\n",
    "        x = F.relu(self.critic_fc(combined))\n",
    "        value = self.critic_out(x)\n",
    "        return value\n",
    "    \n",
    "    def get_actions_and_value(self, observations, training=True):\n",
    "        \"\"\"CTDE: Centralized training, Decentralized execution\"\"\"\n",
    "        batch_size = observations[0]['grid'].shape[0]\n",
    "        \n",
    "        # Generate messages\n",
    "        messages_all = []\n",
    "        if self.use_communication and training:\n",
    "            for obs in observations:\n",
    "                state = obs['state']\n",
    "                message = self.comm.encode_message(state)\n",
    "                messages_all.append(message)\n",
    "        \n",
    "        # Process each agent\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        features_all = []\n",
    "        \n",
    "        for i, obs in enumerate(observations):\n",
    "            grid = obs['grid'].permute(0, 3, 1, 2)\n",
    "            state = obs['state']\n",
    "            \n",
    "            # Aggregate messages from other agents\n",
    "            if self.use_communication and training:\n",
    "                other_messages = [messages_all[j] for j in range(self.n_agents) if j != i]\n",
    "                if other_messages:\n",
    "                    aggregated_msg = torch.stack(other_messages).mean(dim=0)\n",
    "                else:\n",
    "                    aggregated_msg = torch.zeros(batch_size, self.comm.message_dim).to(grid.device)\n",
    "            else:\n",
    "                aggregated_msg = None\n",
    "            \n",
    "            # Get policy\n",
    "            logits, features = self.forward_actor(grid, state, aggregated_msg)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            \n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            entropies.append(entropy)\n",
    "            features_all.append(features)\n",
    "        \n",
    "        # Centralized value\n",
    "        if training:\n",
    "            value = self.forward_critic(features_all)\n",
    "        else:\n",
    "            value = None\n",
    "        \n",
    "        return actions, log_probs, entropies, value\n",
    "\n",
    "print(\"‚úì Multi-Agent Actor-Critic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d33584",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Multi-Agent PPO\n",
    "\n",
    "PPO extended for multi-agent coordination with:\n",
    "- Shared parameters across agents\n",
    "- Communication between agents  \n",
    "- Cooperative reward shaping\n",
    "- Centralized critic, decentralized actors (CTDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full MultiAgentPPO class implementation from the original code\n",
    "# This includes select_actions, cooperative rewards, store_transition, and update methods\n",
    "\n",
    "class MultiAgentPPO:\n",
    "    \"\"\"PPO for multi-agent coordination with communication and cooperative rewards\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_shape, state_dim, action_dim, n_agents, learning_rate=3e-4,\n",
    "                 gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, entropy_coef=0.01,\n",
    "                 value_coef=0.5, cooperation_bonus=2.0, use_communication=True):\n",
    "        self.n_agents = n_agents\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.cooperation_bonus = cooperation_bonus\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.network = MultiAgentActorCritic(\n",
    "            grid_shape, state_dim, action_dim, n_agents, use_communication=use_communication\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.rollout_buffer = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        print(f\"‚úì Multi-Agent PPO initialized ({n_agents} agents, comm={use_communication})\")\n",
    "    \n",
    "    def select_actions(self, observations):\n",
    "        \"\"\"Select actions for all agents\"\"\"\n",
    "        obs_tensors = []\n",
    "        for obs in observations:\n",
    "            grid = torch.FloatTensor(obs['grid']).unsqueeze(0).to(self.device)\n",
    "            state = torch.FloatTensor(obs['state']).unsqueeze(0).to(self.device)\n",
    "            obs_tensors.append({'grid': grid, 'state': state})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions, log_probs, entropies, value = self.network.get_actions_and_value(obs_tensors, training=True)\n",
    "        \n",
    "        actions_np = [a.item() for a in actions]\n",
    "        log_probs_np = [lp.item() for lp in log_probs]\n",
    "        value_np = value.item() if value is not None else 0.0\n",
    "        \n",
    "        return actions_np, log_probs_np, value_np\n",
    "    \n",
    "    def compute_cooperative_reward(self, individual_rewards, agents_alive):\n",
    "        \"\"\"Shape rewards to encourage cooperation\"\"\"\n",
    "        n_alive = sum(agents_alive)\n",
    "        team_bonus = self.cooperation_bonus * (n_alive / self.n_agents)\n",
    "        \n",
    "        shaped_rewards = []\n",
    "        for reward, alive in zip(individual_rewards, agents_alive):\n",
    "            if alive:\n",
    "                shaped_rewards.append(reward + team_bonus)\n",
    "            else:\n",
    "                shaped_rewards.append(reward)\n",
    "        \n",
    "        return shaped_rewards\n",
    "    \n",
    "    def store_transition(self, observations, actions, rewards, log_probs, value, dones):\n",
    "        \"\"\"Store multi-agent transition\"\"\"\n",
    "        self.rollout_buffer.append({\n",
    "            'observations': observations,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'log_probs': log_probs,\n",
    "            'value': value,\n",
    "            'dones': dones\n",
    "        })\n",
    "    \n",
    "    def update(self, n_epochs=4):\n",
    "        \"\"\"PPO update (simplified version - see full implementation in code)\"\"\"\n",
    "        if len(self.rollout_buffer) < 32:\n",
    "            return None\n",
    "        \n",
    "        # Implementation includes GAE computation, batch processing, and PPO loss\n",
    "        # See full code for complete implementation\n",
    "        \n",
    "        avg_loss = 0.0  # Placeholder\n",
    "        self.loss_history.append(avg_loss)\n",
    "        self.rollout_buffer = []\n",
    "        \n",
    "        return avg_loss\n",
    "\n",
    "print(\"‚úì Multi-Agent PPO defined (simplified - see source for full implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7310947",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training & Visualization\n",
    "\n",
    "Functions for training multi-agent systems and visualizing cooperative behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiagent(env, agent, n_episodes=200):\n",
    "    \"\"\"Train multi-agent system with cooperation metrics\"\"\"\n",
    "    episode_rewards = []\n",
    "    survival_rates = []\n",
    "    cooperation_scores = []\n",
    "    \n",
    "    print(f\"\\nüöÄ Training Multi-Agent System for {n_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        observations, info = env.reset()\n",
    "        done_flags = [False] * env.config.n_agents\n",
    "        episode_reward = [0.0] * env.config.n_agents\n",
    "        step_count = 0\n",
    "        \n",
    "        while not all(done_flags) and step_count < env.config.max_steps:\n",
    "            actions, log_probs, value = agent.select_actions(observations)\n",
    "            next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "            \n",
    "            agents_alive = [not (d or t) for d, t in zip(dones, truncated)]\n",
    "            shaped_rewards = agent.compute_cooperative_reward(rewards, agents_alive)\n",
    "            \n",
    "            agent.store_transition(observations, actions, shaped_rewards, log_probs, value,\n",
    "                                 [d or t for d, t in zip(dones, truncated)])\n",
    "            \n",
    "            for i in range(env.config.n_agents):\n",
    "                episode_reward[i] += shaped_rewards[i]\n",
    "                done_flags[i] = dones[i] or truncated[i]\n",
    "            \n",
    "            observations = next_observations\n",
    "            step_count += 1\n",
    "        \n",
    "        if episode % 5 == 0:\n",
    "            loss = agent.update(n_epochs=4)\n",
    "        \n",
    "        mean_reward = np.mean(episode_reward)\n",
    "        survival_rate = sum(not d for d in done_flags) / len(done_flags)\n",
    "        cooperation_score = mean_reward * survival_rate\n",
    "        \n",
    "        episode_rewards.append(mean_reward)\n",
    "        survival_rates.append(survival_rate)\n",
    "        cooperation_scores.append(cooperation_score)\n",
    "        \n",
    "        if (episode + 1) % 20 == 0:\n",
    "            print(f\"Episode {episode+1} | Reward: {mean_reward:.2f} | Survival: {survival_rate:.2%} | Coop: {cooperation_score:.2f}\")\n",
    "    \n",
    "    return {'rewards': episode_rewards, 'survival_rates': survival_rates, 'cooperation_scores': cooperation_scores}\n",
    "\n",
    "def plot_multiagent_results(results):\n",
    "    \"\"\"Plot multi-agent training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Mean rewards\n",
    "    rewards = results['rewards']\n",
    "    axes[0, 0].plot(rewards, alpha=0.3)\n",
    "    if len(rewards) > 20:\n",
    "        smoothed = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
    "        axes[0, 0].plot(range(19, len(rewards)), smoothed, linewidth=2)\n",
    "    axes[0, 0].set_title('Multi-Agent Mean Rewards')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Survival rates\n",
    "    survival = results['survival_rates']\n",
    "    axes[0, 1].plot(survival, alpha=0.3)\n",
    "    if len(survival) > 20:\n",
    "        smoothed = np.convolve(survival, np.ones(20)/20, mode='valid')\n",
    "        axes[0, 1].plot(range(19, len(survival)), smoothed, linewidth=2)\n",
    "    axes[0, 1].set_title('Agent Survival Rate')\n",
    "    axes[0, 1].set_ylim([0, 1.1])\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cooperation scores\n",
    "    coop = results['cooperation_scores']\n",
    "    axes[1, 0].plot(coop, alpha=0.3)\n",
    "    if len(coop) > 20:\n",
    "        smoothed = np.convolve(coop, np.ones(20)/20, mode='valid')\n",
    "        axes[1, 0].plot(range(19, len(coop)), smoothed, linewidth=2)\n",
    "    axes[1, 0].set_title('Cooperation Score (Reward √ó Survival)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution (last 50 episodes)\n",
    "    axes[1, 1].hist(rewards[-50:], alpha=0.5, bins=20, label='Rewards')\n",
    "    axes[1, 1].hist(coop[-50:], alpha=0.5, bins=20, label='Cooperation')\n",
    "    axes[1, 1].set_title('Score Distribution (Last 50 Episodes)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Multi-Agent Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Training and visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08aca6",
   "metadata": {},
   "source": [
    "## ‚úÖ Multi-Agent RL Ready!\n",
    "\n",
    "**Key Concepts Implemented:**\n",
    "- ‚úì Multi-agent actor-critic architecture\n",
    "- ‚úì Agent communication network\n",
    "- ‚úì Centralized training, decentralized execution (CTDE)\n",
    "- ‚úì Cooperative reward shaping\n",
    "- ‚úì Parameter sharing across agents\n",
    "\n",
    "**Experiments to Try:**\n",
    "1. With vs without communication\n",
    "2. Different cooperation bonus values\n",
    "3. Competitive vs cooperative reward structures\n",
    "4. Emergent behaviors and specialization\n",
    "\n",
    "**See example usage in code comments below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0acbb",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Communication Module\n",
    "\n",
    "Learned communication between agents - each agent can send/receive messages to nearby agents.\n",
    "\n",
    "**RL Concept:** Communication as part of the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pettingzoo supersuit torch -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "print(\"‚úì Multi-agent libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dca89e",
   "metadata": {},
   "source": [
    "## üì¶ Setup - Install Dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ce005",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# THE AUTONOMOUS COLONY - PART 3: MULTI-AGENT RL\n",
    "# Cooperation, Competition, and Emergent Behaviors\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "# ðŸ¤ The Autonomous Colony - Multi-Agent Coordination\n",
    "\n",
    "**RL Concepts Covered:**\n",
    "1. Multi-Agent RL (MARL) fundamentals\n",
    "2. Communication between agents\n",
    "3. Centralized training, decentralized execution (CTDE)\n",
    "4. Reward shaping for cooperation\n",
    "5. Nash equilibria and social dilemmas\n",
    "6. Parameter sharing vs independent learners\n",
    "7. PettingZoo integration\n",
    "\n",
    "**Prerequisites:**\n",
    "- Parts 1 & 2 (environment and single-agent RL)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# !pip install pettingzoo supersuit torch -q\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "print(\"âœ“ Multi-agent libraries loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. COMMUNICATION MODULE\n",
    "# ============================================================================\n",
    "\n",
    "class CommunicationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned communication between agents.\n",
    "    Each agent can send/receive messages to nearby agents.\n",
    "    \n",
    "    RL Concept: Communication as part of the action space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, message_dim: int = 16, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.message_dim = message_dim\n",
    "        \n",
    "        # Message encoder (from agent state to message)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(5, hidden_dim),  # 5 = agent state features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, message_dim)\n",
    "        )\n",
    "        \n",
    "        # Message aggregator (combine received messages)\n",
    "        self.aggregator = nn.Sequential(\n",
    "            nn.Linear(message_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "    \n",
    "    def encode_message(self, agent_state):\n",
    "        \"\"\"Generate message from agent state\"\"\"\n",
    "        return self.encoder(agent_state)\n",
    "    \n",
    "    def aggregate_messages(self, messages):\n",
    "        \"\"\"Aggregate messages from multiple agents\"\"\"\n",
    "        if len(messages) == 0:\n",
    "            return torch.zeros(1, self.message_dim)\n",
    "        \n",
    "        # Mean pooling\n",
    "        combined = torch.stack(messages).mean(dim=0)\n",
    "        return self.aggregator(combined)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MULTI-AGENT ACTOR-CRITIC\n",
    "# ============================================================================\n",
    "\n",
    "class MultiAgentActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic with communication for multi-agent settings.\n",
    "    \n",
    "    Architecture:\n",
    "    - Individual observations + messages -> features\n",
    "    - Actor: decentralized policy per agent\n",
    "    - Critic: can use global state (CTDE)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_shape: tuple,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        n_agents: int,\n",
    "        message_dim: int = 16,\n",
    "        use_communication: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.use_communication = use_communication\n",
    "        \n",
    "        # Communication module\n",
    "        if use_communication:\n",
    "            self.comm = CommunicationNetwork(message_dim)\n",
    "            extra_dim = message_dim\n",
    "        else:\n",
    "            extra_dim = 0\n",
    "        \n",
    "        # Individual feature extractors (decentralized)\n",
    "        self.conv1 = nn.Conv2d(grid_shape[2], 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        conv_out = grid_shape[0] * grid_shape[1] * 64\n",
    "        \n",
    "        self.state_fc = nn.Linear(state_dim, 64)\n",
    "        self.feature_fc = nn.Linear(conv_out + 64 + extra_dim, 256)\n",
    "        \n",
    "        # Actor (decentralized - per agent policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Critic (centralized - global value)\n",
    "        # Takes features from all agents\n",
    "        self.critic_fc = nn.Linear(256 * n_agents, 256)\n",
    "        self.critic_out = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward_actor(self, grid_obs, state_obs, messages=None):\n",
    "        \"\"\"Forward pass for actor (decentralized)\"\"\"\n",
    "        # Extract features\n",
    "        x = F.relu(self.conv1(grid_obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        s = F.relu(self.state_fc(state_obs))\n",
    "        \n",
    "        # Add communication\n",
    "        if self.use_communication and messages is not None:\n",
    "            features = torch.cat([x, s, messages], dim=1)\n",
    "        else:\n",
    "            features = torch.cat([x, s], dim=1)\n",
    "        \n",
    "        features = F.relu(self.feature_fc(features))\n",
    "        logits = self.actor(features)\n",
    "        \n",
    "        return logits, features\n",
    "    \n",
    "    def forward_critic(self, all_agent_features):\n",
    "        \"\"\"Forward pass for critic (centralized)\"\"\"\n",
    "        # Concatenate all agent features\n",
    "        combined = torch.cat(all_agent_features, dim=1)\n",
    "        x = F.relu(self.critic_fc(combined))\n",
    "        value = self.critic_out(x)\n",
    "        return value\n",
    "    \n",
    "    def get_actions_and_value(self, observations, training=True):\n",
    "        \"\"\"\n",
    "        Get actions for all agents and global value.\n",
    "        \n",
    "        CTDE: Centralized training (uses all observations for value),\n",
    "              Decentralized execution (each agent acts on local obs)\n",
    "        \"\"\"\n",
    "        batch_size = observations[0]['grid'].shape[0]\n",
    "        \n",
    "        # Step 1: Generate messages (if using communication)\n",
    "        messages_all = []\n",
    "        if self.use_communication and training:\n",
    "            for obs in observations:\n",
    "                state = obs['state']\n",
    "                message = self.comm.encode_message(state)\n",
    "                messages_all.append(message)\n",
    "        \n",
    "        # Step 2: Process each agent\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        features_all = []\n",
    "        \n",
    "        for i, obs in enumerate(observations):\n",
    "            grid = obs['grid'].permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "            state = obs['state']\n",
    "            \n",
    "            # Aggregate messages from other agents\n",
    "            if self.use_communication and training:\n",
    "                # Simple: aggregate all other agents' messages\n",
    "                other_messages = [messages_all[j] for j in range(self.n_agents) if j != i]\n",
    "                if other_messages:\n",
    "                    aggregated_msg = torch.stack(other_messages).mean(dim=0)\n",
    "                else:\n",
    "                    aggregated_msg = torch.zeros(batch_size, self.comm.message_dim).to(grid.device)\n",
    "            else:\n",
    "                aggregated_msg = None\n",
    "            \n",
    "            # Get policy\n",
    "            logits, features = self.forward_actor(grid, state, aggregated_msg)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            \n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            entropies.append(entropy)\n",
    "            features_all.append(features)\n",
    "        \n",
    "        # Step 3: Centralized value (uses all agent features)\n",
    "        if training:\n",
    "            value = self.forward_critic(features_all)\n",
    "        else:\n",
    "            value = None\n",
    "        \n",
    "        return actions, log_probs, entropies, value\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MULTI-AGENT PPO\n",
    "# ============================================================================\n",
    "\n",
    "class MultiAgentPPO:\n",
    "    \"\"\"\n",
    "    PPO for multi-agent coordination.\n",
    "    \n",
    "    Key features:\n",
    "    - Shared parameters across agents (faster learning)\n",
    "    - Communication between agents\n",
    "    - Cooperative reward shaping\n",
    "    - Centralized critic, decentralized actors (CTDE)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_shape: tuple,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        n_agents: int,\n",
    "        learning_rate: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        cooperation_bonus: float = 2.0,\n",
    "        use_communication: bool = True\n",
    "    ):\n",
    "        self.n_agents = n_agents\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.cooperation_bonus = cooperation_bonus\n",
    "        \n",
    "        # Network\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.network = MultiAgentActorCritic(\n",
    "            grid_shape, state_dim, action_dim, n_agents,\n",
    "            use_communication=use_communication\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Rollout buffer\n",
    "        self.rollout_buffer = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        print(f\"âœ“ Multi-Agent PPO initialized ({n_agents} agents, comm={use_communication})\")\n",
    "    \n",
    "    def select_actions(self, observations: List[Dict]):\n",
    "        \"\"\"Select actions for all agents\"\"\"\n",
    "        # Convert observations to tensors\n",
    "        obs_tensors = []\n",
    "        for obs in observations:\n",
    "            grid = torch.FloatTensor(obs['grid']).unsqueeze(0).to(self.device)\n",
    "            state = torch.FloatTensor(obs['state']).unsqueeze(0).to(self.device)\n",
    "            obs_tensors.append({'grid': grid, 'state': state})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions, log_probs, entropies, value = self.network.get_actions_and_value(\n",
    "                obs_tensors, training=True\n",
    "            )\n",
    "        \n",
    "        actions_np = [a.item() for a in actions]\n",
    "        log_probs_np = [lp.item() for lp in log_probs]\n",
    "        value_np = value.item() if value is not None else 0.0\n",
    "        \n",
    "        return actions_np, log_probs_np, value_np\n",
    "    \n",
    "    def compute_cooperative_reward(self, individual_rewards: List[float], \n",
    "                                   agents_alive: List[bool]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Shape rewards to encourage cooperation.\n",
    "        \n",
    "        Strategies:\n",
    "        - Team bonus: reward when all agents survive\n",
    "        - Proximity bonus: reward agents near each other\n",
    "        - Resource sharing: higher reward when resources distributed\n",
    "        \"\"\"\n",
    "        n_alive = sum(agents_alive)\n",
    "        team_bonus = self.cooperation_bonus * (n_alive / self.n_agents)\n",
    "        \n",
    "        shaped_rewards = []\n",
    "        for i, (reward, alive) in enumerate(zip(individual_rewards, agents_alive)):\n",
    "            if alive:\n",
    "                # Add team survival bonus\n",
    "                shaped_rewards.append(reward + team_bonus)\n",
    "            else:\n",
    "                shaped_rewards.append(reward)\n",
    "        \n",
    "        return shaped_rewards\n",
    "    \n",
    "    def store_transition(self, observations, actions, rewards, log_probs, value, dones):\n",
    "        \"\"\"Store multi-agent transition\"\"\"\n",
    "        self.rollout_buffer.append({\n",
    "            'observations': observations,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'log_probs': log_probs,\n",
    "            'value': value,\n",
    "            'dones': dones\n",
    "        })\n",
    "    \n",
    "    def update(self, n_epochs: int = 4):\n",
    "        \"\"\"PPO update for multi-agent setting\"\"\"\n",
    "        if len(self.rollout_buffer) < 32:\n",
    "            return None\n",
    "        \n",
    "        # Extract data\n",
    "        all_obs = [t['observations'] for t in self.rollout_buffer]\n",
    "        all_actions = [t['actions'] for t in self.rollout_buffer]\n",
    "        all_rewards = [t['rewards'] for t in self.rollout_buffer]\n",
    "        all_log_probs = [t['log_probs'] for t in self.rollout_buffer]\n",
    "        all_values = [t['value'] for t in self.rollout_buffer]\n",
    "        all_dones = [t['dones'] for t in self.rollout_buffer]\n",
    "        \n",
    "        # Compute advantages (using team value)\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(all_rewards))):\n",
    "            # Use mean reward across agents\n",
    "            mean_reward = np.mean(all_rewards[t])\n",
    "            mean_done = any(all_dones[t])\n",
    "            \n",
    "            if t == len(all_rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = all_values[t + 1]\n",
    "            \n",
    "            delta = mean_reward + self.gamma * next_value * (1 - mean_done) - all_values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - mean_done) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + all_values[t])\n",
    "        \n",
    "        # Normalize\n",
    "        advantages = np.array(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Convert to tensors (flattened across agents)\n",
    "        obs_grids = []\n",
    "        obs_states = []\n",
    "        actions_flat = []\n",
    "        old_log_probs_flat = []\n",
    "        \n",
    "        for t in range(len(all_obs)):\n",
    "            for agent_id in range(self.n_agents):\n",
    "                obs = all_obs[t][agent_id]\n",
    "                obs_grids.append(torch.FloatTensor(obs['grid']))\n",
    "                obs_states.append(torch.FloatTensor(obs['state']))\n",
    "                actions_flat.append(all_actions[t][agent_id])\n",
    "                old_log_probs_flat.append(all_log_probs[t][agent_id])\n",
    "        \n",
    "        # Batch tensors\n",
    "        grids_batch = torch.stack(obs_grids).to(self.device)\n",
    "        states_batch = torch.stack(obs_states).to(self.device)\n",
    "        actions_batch = torch.LongTensor(actions_flat).to(self.device)\n",
    "        old_log_probs_batch = torch.FloatTensor(old_log_probs_flat).to(self.device)\n",
    "        advantages_batch = torch.FloatTensor(advantages).repeat_interleave(self.n_agents).to(self.device)\n",
    "        returns_batch = torch.FloatTensor(returns).repeat_interleave(self.n_agents).to(self.device)\n",
    "        \n",
    "        # PPO epochs\n",
    "        total_loss = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            # Forward pass for all agents\n",
    "            # (Simplified: process in batch rather than grouping by timestep)\n",
    "            logits, features = self.network.forward_actor(\n",
    "                grids_batch.permute(0, 3, 1, 2),\n",
    "                states_batch\n",
    "            )\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions_batch)\n",
    "            entropy = dist.entropy()\n",
    "            \n",
    "            # Value function (simplified: use per-agent value)\n",
    "            values_pred = self.network.forward_critic([features] * self.n_agents).squeeze()\n",
    "            \n",
    "            # PPO loss\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs_batch)\n",
    "            surr1 = ratio * advantages_batch\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages_batch\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            value_loss = F.mse_loss(values_pred, returns_batch)\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / n_epochs\n",
    "        self.loss_history.append(avg_loss)\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.rollout_buffer = []\n",
    "        \n",
    "        return avg_loss\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TRAINING LOOP FOR MULTI-AGENT\n",
    "# ============================================================================\n",
    "\n",
    "def train_multiagent(env, agent, n_episodes: int = 200):\n",
    "    \"\"\"Train multi-agent system\"\"\"\n",
    "    episode_rewards = []\n",
    "    survival_rates = []\n",
    "    cooperation_scores = []\n",
    "    \n",
    "    print(f\"\\nðŸš€ Training Multi-Agent System for {n_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        observations, info = env.reset()\n",
    "        done_flags = [False] * env.config.n_agents\n",
    "        episode_reward = [0.0] * env.config.n_agents\n",
    "        step_count = 0\n",
    "        \n",
    "        while not all(done_flags) and step_count < env.config.max_steps:\n",
    "            # Get actions for all agents\n",
    "            actions, log_probs, value = agent.select_actions(observations)\n",
    "            \n",
    "            # Step environment\n",
    "            next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "            \n",
    "            # Compute cooperative rewards\n",
    "            agents_alive = [not (d or t) for d, t in zip(dones, truncated)]\n",
    "            shaped_rewards = agent.compute_cooperative_reward(rewards, agents_alive)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(\n",
    "                observations, actions, shaped_rewards, log_probs, value,\n",
    "                [d or t for d, t in zip(dones, truncated)]\n",
    "            )\n",
    "            \n",
    "            # Update episode rewards\n",
    "            for i in range(env.config.n_agents):\n",
    "                episode_reward[i] += shaped_rewards[i]\n",
    "                done_flags[i] = dones[i] or truncated[i]\n",
    "            \n",
    "            observations = next_observations\n",
    "            step_count += 1\n",
    "        \n",
    "        # Update agent\n",
    "        if episode % 5 == 0:  # Update every 5 episodes\n",
    "            loss = agent.update(n_epochs=4)\n",
    "            if loss:\n",
    "                print(f\"  Update loss: {loss:.4f}\")\n",
    "        \n",
    "        # Metrics\n",
    "        mean_reward = np.mean(episode_reward)\n",
    "        survival_rate = sum(not d for d in done_flags) / len(done_flags)\n",
    "        cooperation_score = mean_reward * survival_rate  # Combined metric\n",
    "        \n",
    "        episode_rewards.append(mean_reward)\n",
    "        survival_rates.append(survival_rate)\n",
    "        cooperation_scores.append(cooperation_score)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:])\n",
    "            avg_survival = np.mean(survival_rates[-20:])\n",
    "            avg_coop = np.mean(cooperation_scores[-20:])\n",
    "            print(f\"\\nEpisode {episode+1}/{n_episodes}\")\n",
    "            print(f\"  Avg Reward: {avg_reward:.2f}\")\n",
    "            print(f\"  Avg Survival: {avg_survival:.2%}\")\n",
    "            print(f\"  Cooperation Score: {avg_coop:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'survival_rates': survival_rates,\n",
    "        'cooperation_scores': cooperation_scores\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 5. VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_multiagent_results(results: Dict):\n",
    "    \"\"\"Plot multi-agent training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Mean rewards\n",
    "    rewards = results['rewards']\n",
    "    axes[0, 0].plot(rewards, alpha=0.3, label='Raw')\n",
    "    if len(rewards) > 20:\n",
    "        smoothed = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
    "        axes[0, 0].plot(range(19, len(rewards)), smoothed, linewidth=2, label='Smoothed')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Mean Reward')\n",
    "    axes[0, 0].set_title('Multi-Agent Mean Rewards')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Survival rates\n",
    "    survival = results['survival_rates']\n",
    "    axes[0, 1].plot(survival, alpha=0.3, label='Raw')\n",
    "    if len(survival) > 20:\n",
    "        smoothed = np.convolve(survival, np.ones(20)/20, mode='valid')\n",
    "        axes[0, 1].plot(range(19, len(survival)), smoothed, linewidth=2, label='Smoothed')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Survival Rate')\n",
    "    axes[0, 1].set_title('Agent Survival Rate')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 1.1])\n",
    "    \n",
    "    # Cooperation scores\n",
    "    coop = results['cooperation_scores']\n",
    "    axes[1, 0].plot(coop, alpha=0.3, label='Raw')\n",
    "    if len(coop) > 20:\n",
    "        smoothed = np.convolve(coop, np.ones(20)/20, mode='valid')\n",
    "        axes[1, 0].plot(range(19, len(coop)), smoothed, linewidth=2, label='Smoothed')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Cooperation Score')\n",
    "    axes[1, 0].set_title('Cooperation Score (Reward Ã— Survival)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution comparison (final 50 episodes)\n",
    "    axes[1, 1].hist(rewards[-50:], alpha=0.5, bins=20, label='Rewards')\n",
    "    axes[1, 1].hist(coop[-50:], alpha=0.5, bins=20, label='Cooperation')\n",
    "    axes[1, 1].set_xlabel('Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Score Distribution (Last 50 Episodes)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Multi-Agent Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO & EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-AGENT RL MODULE READY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š Key Concepts Implemented:\")\n",
    "print(\"  âœ“ Multi-agent actor-critic architecture\")\n",
    "print(\"  âœ“ Agent communication network\")\n",
    "print(\"  âœ“ Centralized training, decentralized execution (CTDE)\")\n",
    "print(\"  âœ“ Cooperative reward shaping\")\n",
    "print(\"  âœ“ Parameter sharing across agents\")\n",
    "print(\"\\nðŸ§ª Experiments to try:\")\n",
    "print(\"  1. With vs without communication\")\n",
    "print(\"  2. Different cooperation bonus values\")\n",
    "print(\"  3. Competitive vs cooperative reward structures\")\n",
    "print(\"  4. Emergent behaviors and specialization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "\"\"\"\n",
    "# Create environment\n",
    "from part1_environment import ColonyEnvironment, EnvironmentConfig\n",
    "\n",
    "config = EnvironmentConfig(\n",
    "    grid_size=20,\n",
    "    n_agents=4,\n",
    "    max_steps=300\n",
    ")\n",
    "env = ColonyEnvironment(config)\n",
    "\n",
    "# Create multi-agent PPO\n",
    "ma_agent = MultiAgentPPO(\n",
    "    grid_shape=(7, 7, 5),  # observation_radius=3\n",
    "    state_dim=5,\n",
    "    action_dim=9,\n",
    "    n_agents=4,\n",
    "    use_communication=True,\n",
    "    cooperation_bonus=3.0\n",
    ")\n",
    "\n",
    "# Train\n",
    "results = train_multiagent(env, ma_agent, n_episodes=200)\n",
    "plot_multiagent_results(results)\n",
    "\n",
    "# Visualize trained agents\n",
    "env.render()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfb1000",
   "metadata": {},
   "source": [
    "# ü§ñ The Autonomous Colony - GPU Training on Google Colab\n",
    "\n",
    "Train RL agents with GPU acceleration.\n",
    "\n",
    "**Setup:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "2. Run all cells in order\n",
    "3. Models saved to Google Drive\n",
    "4. Download for local visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e819a73",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "MODEL_DIR = '/content/drive/MyDrive/autonomous_colony_models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"‚úì Models ‚Üí {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e123283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/ritikkumarv/autonomous-colony.git\n",
    "%cd autonomous-colony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy matplotlib seaborn\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4feb5",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'n_agents': 2,           # Start with 2 agents\n",
    "    'grid_size': 20,         # 20x20 grid\n",
    "    'n_episodes': 500,       # 500 episodes (~1-2 hours on T4)\n",
    "    'max_steps': 200,        # Steps per episode\n",
    "    'save_interval': 100,    # Save every 100 episodes\n",
    "    'agent_type': 'ppo',     # 'ppo' or 'dqn'\n",
    "}\n",
    "\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154af82",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16987992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/autonomous-colony')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.environment import ColonyEnvironment\n",
    "from src.agents import PPOAgent, DQNAgent\n",
    "\n",
    "# Create environment\n",
    "env = ColonyEnvironment(\n",
    "    n_agents=CONFIG['n_agents'],\n",
    "    grid_size=CONFIG['grid_size']\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "if CONFIG['agent_type'] == 'ppo':\n",
    "    agent = PPOAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=3e-4,\n",
    "        n_epochs=10,\n",
    "        batch_size=64\n",
    "    )\n",
    "else:  # dqn\n",
    "    agent = DQNAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úì {CONFIG['agent_type'].upper()} Agent ready\")\n",
    "print(f\"‚úì Device: {agent.device}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING STARTED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f82c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "success_rates = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for episode in range(CONFIG['n_episodes']):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Episode rollout\n",
    "    while not done and step < CONFIG['max_steps']:\n",
    "        actions = []\n",
    "        \n",
    "        # Select actions for each agent\n",
    "        for obs in observations:\n",
    "            if CONFIG['agent_type'] == 'ppo':\n",
    "                action, log_prob, value = agent.select_action(obs, training=True)\n",
    "                \n",
    "                # Store in rollout buffer\n",
    "                agent.rollout_buffer.append({\n",
    "                    'observation': obs,\n",
    "                    'action': action,\n",
    "                    'log_prob': log_prob,\n",
    "                    'value': value\n",
    "                })\n",
    "            else:  # DQN\n",
    "                action = agent.select_action(obs, training=True)\n",
    "            \n",
    "            actions.append(action)\n",
    "        \n",
    "        # Environment step\n",
    "        next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "        # Store rewards and next states\n",
    "        if CONFIG['agent_type'] == 'ppo':\n",
    "            # Update last entries in rollout buffer with rewards and next observations\n",
    "            for i, (reward, next_obs, done_flag) in enumerate(zip(rewards, next_observations, dones)):\n",
    "                idx = -(len(observations) - i)\n",
    "                agent.rollout_buffer[idx]['reward'] = reward\n",
    "                agent.rollout_buffer[idx]['next_observation'] = next_obs\n",
    "                agent.rollout_buffer[idx]['done'] = done_flag\n",
    "        else:  # DQN\n",
    "            for obs, action, reward, next_obs, done_flag in zip(\n",
    "                observations, actions, rewards, next_observations, dones\n",
    "            ):\n",
    "                agent.memory.push(obs, action, reward, next_obs, done_flag)\n",
    "        \n",
    "        episode_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    # Update agent\n",
    "    if CONFIG['agent_type'] == 'ppo':\n",
    "        if len(agent.rollout_buffer) >= agent.batch_size:\n",
    "            agent.update()\n",
    "    else:  # DQN\n",
    "        if len(agent.memory) >= agent.batch_size:\n",
    "            agent.update()\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step)\n",
    "    success_rates.append(1 if episode_reward > 0 else 0)\n",
    "    \n",
    "    # Live plotting every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Rewards\n",
    "        axes[0, 0].clear()\n",
    "        axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue')\n",
    "        if len(episode_rewards) >= 50:\n",
    "            ma = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "            axes[0, 0].plot(range(49, len(episode_rewards)), ma, 'r-', linewidth=2)\n",
    "        axes[0, 0].set_title('Episode Rewards')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Lengths\n",
    "        axes[0, 1].clear()\n",
    "        axes[0, 1].plot(episode_lengths, alpha=0.6, color='green')\n",
    "        axes[0, 1].set_title('Episode Lengths')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Steps')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Success rate\n",
    "        axes[1, 0].clear()\n",
    "        if len(success_rates) >= 50:\n",
    "            sr = np.convolve(success_rates, np.ones(50)/50, mode='valid')\n",
    "            axes[1, 0].plot(range(49, len(success_rates)), sr, 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Success Rate (50-ep avg)')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Success Rate')\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Reward distribution\n",
    "        axes[1, 1].clear()\n",
    "        recent = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
    "        axes[1, 1].hist(recent, bins=20, alpha=0.7, color='blue')\n",
    "        axes[1, 1].set_title('Recent Reward Distribution')\n",
    "        axes[1, 1].set_xlabel('Reward')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{CONFIG['n_episodes']}\")\n",
    "        print(f\"  Reward: {episode_reward:.2f}\")\n",
    "        print(f\"  Steps: {step}\")\n",
    "        print(f\"  Avg Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "        print(f\"  Success Rate (last 100): {np.mean(success_rates[-100:]):.1%}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % CONFIG['save_interval'] == 0:\n",
    "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        path = f\"{MODEL_DIR}/{CONFIG['agent_type']}_ep{episode+1}_{ts}.pt\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'episode': episode + 1,\n",
    "            'config': CONFIG,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'success_rates': success_rates\n",
    "        }\n",
    "        \n",
    "        if CONFIG['agent_type'] == 'ppo':\n",
    "            checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        else:  # DQN\n",
    "            checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "            checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"\\nüíæ Checkpoint: {path}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Avg Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Success Rate (last 100): {np.mean(success_rates[-100:]):.1%}\")\n",
    "print(f\"Best Reward: {max(episode_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19581d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c689c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "final_path = f\"{MODEL_DIR}/{CONFIG['agent_type']}_final_{ts}.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    'episode': CONFIG['n_episodes'],\n",
    "    'config': CONFIG,\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'episode_lengths': episode_lengths,\n",
    "    'success_rates': success_rates,\n",
    "    'final_stats': {\n",
    "        'avg_reward': np.mean(episode_rewards[-100:]),\n",
    "        'success_rate': np.mean(success_rates[-100:]),\n",
    "        'best_reward': max(episode_rewards)\n",
    "    }\n",
    "}\n",
    "\n",
    "if CONFIG['agent_type'] == 'ppo':\n",
    "    checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "else:  # DQN\n",
    "    checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "    checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "\n",
    "torch.save(checkpoint, final_path)\n",
    "\n",
    "print(f\"‚úÖ Final model: {final_path}\")\n",
    "print(f\"\\nTo visualize locally:\")\n",
    "print(f\"1. Download from Google Drive: {MODEL_DIR}\")\n",
    "print(f\"2. Run: python visualize.py --model models/{CONFIG['agent_type']}_final_{ts}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0246caf",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ced95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved models\n",
    "import os\n",
    "models = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pt')]\n",
    "models.sort()\n",
    "\n",
    "print(f\"Saved Models ({len(models)}):\")\n",
    "print(\"=\"*60)\n",
    "for i, model in enumerate(models, 1):\n",
    "    path = os.path.join(MODEL_DIR, model)\n",
    "    size = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"{i}. {model} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüì• Download from: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14442d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download all models\n",
    "import shutil\n",
    "\n",
    "zip_name = f\"colony_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "zip_path = f\"/content/{zip_name}\"\n",
    "\n",
    "shutil.make_archive(zip_path, 'zip', MODEL_DIR)\n",
    "print(f\"‚úÖ Zipped: {zip_path}.zip\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd0d39",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f57796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "test_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while not done and step < 200:\n",
    "        actions = []\n",
    "        for obs in observations:\n",
    "            if CONFIG['agent_type'] == 'ppo':\n",
    "                action, _, _ = agent.select_action(obs, training=False)\n",
    "            else:\n",
    "                action = agent.select_action(obs, training=False)\n",
    "            actions.append(action)\n",
    "        \n",
    "        next_observations, rewards, dones, truncated, _ = env.step(actions)\n",
    "        ep_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    test_rewards.append(ep_reward)\n",
    "    print(f\"Test {ep + 1}: Reward={ep_reward:.2f}, Steps={step}\")\n",
    "\n",
    "print(f\"\\nTest Avg: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

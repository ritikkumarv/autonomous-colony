{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655b82c9",
   "metadata": {},
   "source": [
    "# ü§ñ The Autonomous Colony - GPU Training on Google Colab\n",
    "\n",
    "This notebook trains RL agents with GPU/TPU acceleration on Google Colab.\n",
    "\n",
    "**Features:**\n",
    "- üöÄ GPU/TPU acceleration (T4, A100, V100)\n",
    "- üìä Multiple agents (PPO, DQN, MAPPO)\n",
    "- üß† Advanced RL techniques (Curiosity, Hierarchical, World Models)\n",
    "- üíæ Automatic model saving to Google Drive\n",
    "- üìà Real-time training visualization\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "2. Run all cells in order\n",
    "3. Models will be saved to your Google Drive\n",
    "4. Download trained models for local visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3d73c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for model persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory for models\n",
    "import os\n",
    "MODEL_DIR = '/content/drive/MyDrive/autonomous_colony_models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"‚úì Models will be saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/ritikkumarv/autonomous-colony.git\n",
    "%cd autonomous-colony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision numpy matplotlib seaborn\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3453b",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    # Environment\n",
    "    'n_agents': 4,\n",
    "    'grid_size': 30,\n",
    "    \n",
    "    # Training\n",
    "    'n_episodes': 1000,  # More episodes with GPU\n",
    "    'max_steps': 500,\n",
    "    'save_interval': 100,\n",
    "    \n",
    "    # Agent selection\n",
    "    'agent_type': 'ppo',  # 'ppo', 'dqn', 'mappo'\n",
    "    \n",
    "    # Advanced features (optional)\n",
    "    'use_curiosity': True,\n",
    "    'curiosity_type': 'icm',  # 'icm' or 'rnd'\n",
    "    'use_hierarchical': False,\n",
    "    'use_world_model': False,\n",
    "    'use_curriculum': True,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262074d8",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/autonomous-colony')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.environment import ColonyEnvironment\n",
    "from src.agents import PPOAgent, DQNAgent\n",
    "from src.multiagent import MultiAgentPPO\n",
    "from src.advanced import ICM, RND, CurriculumScheduler\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Create environment\n",
    "env = ColonyEnvironment(\n",
    "    n_agents=TRAINING_CONFIG['n_agents'],\n",
    "    grid_size=TRAINING_CONFIG['grid_size']\n",
    ")\n",
    "\n",
    "# Create agent based on configuration\n",
    "agent_type = TRAINING_CONFIG['agent_type'].lower()\n",
    "\n",
    "if agent_type == 'ppo':\n",
    "    agent = PPOAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=3e-4,\n",
    "        n_epochs=10,\n",
    "        batch_size=128\n",
    "    )\n",
    "    print(\"‚úì Created PPO Agent\")\n",
    "    \n",
    "elif agent_type == 'dqn':\n",
    "    agent = DQNAgent(\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=128,\n",
    "        buffer_size=100000\n",
    "    )\n",
    "    print(\"‚úì Created DQN Agent\")\n",
    "    \n",
    "elif agent_type == 'mappo':\n",
    "    agent = MultiAgentPPO(\n",
    "        n_agents=TRAINING_CONFIG['n_agents'],\n",
    "        grid_shape=(7, 7, 5),\n",
    "        state_dim=5,\n",
    "        action_dim=9,\n",
    "        use_communication=True\n",
    "    )\n",
    "    print(\"‚úì Created Multi-Agent PPO\")\n",
    "\n",
    "# Add curiosity module if enabled\n",
    "curiosity = None\n",
    "if TRAINING_CONFIG['use_curiosity']:\n",
    "    if TRAINING_CONFIG['curiosity_type'] == 'icm':\n",
    "        curiosity = ICM(\n",
    "            grid_shape=(7, 7, 5),\n",
    "            state_dim=5,\n",
    "            action_dim=9\n",
    "        )\n",
    "        print(\"‚úì Added ICM Curiosity Module\")\n",
    "    elif TRAINING_CONFIG['curiosity_type'] == 'rnd':\n",
    "        curiosity = RND(\n",
    "            grid_shape=(7, 7, 5),\n",
    "            state_dim=5\n",
    "        )\n",
    "        print(\"‚úì Added RND Curiosity Module\")\n",
    "\n",
    "# Add curriculum learning if enabled\n",
    "curriculum = None\n",
    "if TRAINING_CONFIG['use_curriculum']:\n",
    "    curriculum = CurriculumScheduler(\n",
    "        initial_difficulty=0.3,\n",
    "        target_difficulty=1.0,\n",
    "        adaptation_rate=0.1\n",
    "    )\n",
    "    print(\"‚úì Added Curriculum Learning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with live visualization\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "success_rates = []\n",
    "curiosity_bonuses = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for episode in range(TRAINING_CONFIG['n_episodes']):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    episode_curiosity = 0\n",
    "    \n",
    "    while not done and step < TRAINING_CONFIG['max_steps']:\n",
    "        # Select actions\n",
    "        if agent_type == 'mappo':\n",
    "            actions, log_probs, values = agent.select_actions(observations, training=True)\n",
    "        else:\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            \n",
    "            for obs in observations:\n",
    "                if agent_type == 'ppo':\n",
    "                    action, log_prob, value = agent.select_action(obs, training=True)\n",
    "                else:  # DQN\n",
    "                    action = agent.select_action(obs, training=True)\n",
    "                    log_prob = 0\n",
    "                    value = 0\n",
    "                    \n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "        \n",
    "        # Step environment\n",
    "        next_observations, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "        # Add curiosity bonus\n",
    "        if curiosity:\n",
    "            for i, (obs, action, next_obs) in enumerate(zip(observations, actions, next_observations)):\n",
    "                bonus = curiosity.compute_bonus(obs, action, next_obs)\n",
    "                rewards[i] += bonus\n",
    "                episode_curiosity += bonus\n",
    "        \n",
    "        # Store transitions\n",
    "        if agent_type == 'mappo':\n",
    "            agent.store_transition(observations, actions, rewards, next_observations, dones, log_probs, values)\n",
    "        elif agent_type == 'ppo':\n",
    "            for i, (obs, action, reward, next_obs, log_prob, value) in enumerate(\n",
    "                zip(observations, actions, rewards, next_observations, log_probs, values)\n",
    "            ):\n",
    "                agent.rollout_buffer.append({\n",
    "                    'observation': obs,\n",
    "                    'action': action,\n",
    "                    'reward': reward,\n",
    "                    'next_observation': next_obs,\n",
    "                    'done': dones[i],\n",
    "                    'log_prob': log_prob,\n",
    "                    'value': value\n",
    "                })\n",
    "        else:  # DQN\n",
    "            for i, (obs, action, reward, next_obs) in enumerate(\n",
    "                zip(observations, actions, rewards, next_observations)\n",
    "            ):\n",
    "                agent.memory.push(obs, action, reward, next_obs, dones[i])\n",
    "        \n",
    "        episode_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    # Update agent\n",
    "    if agent_type == 'ppo':\n",
    "        if len(agent.rollout_buffer) > agent.batch_size:\n",
    "            losses = agent.update()\n",
    "    elif agent_type == 'dqn':\n",
    "        if len(agent.memory) > agent.batch_size:\n",
    "            loss = agent.update()\n",
    "    elif agent_type == 'mappo':\n",
    "        if agent.is_ready_to_update():\n",
    "            losses = agent.update()\n",
    "    \n",
    "    # Update curiosity\n",
    "    if curiosity and hasattr(curiosity, 'update'):\n",
    "        curiosity.update()\n",
    "    \n",
    "    # Update curriculum\n",
    "    if curriculum:\n",
    "        curriculum.update(episode_reward, step)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step)\n",
    "    success = episode_reward > 0\n",
    "    success_rates.append(1 if success else 0)\n",
    "    if curiosity:\n",
    "        curiosity_bonuses.append(episode_curiosity)\n",
    "    \n",
    "    # Live plotting every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Plot rewards\n",
    "        axes[0, 0].clear()\n",
    "        axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue')\n",
    "        if len(episode_rewards) >= 50:\n",
    "            moving_avg = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "            axes[0, 0].plot(range(49, len(episode_rewards)), moving_avg, color='red', linewidth=2)\n",
    "        axes[0, 0].set_title('Episode Rewards')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Total Reward')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot episode lengths\n",
    "        axes[0, 1].clear()\n",
    "        axes[0, 1].plot(episode_lengths, alpha=0.6, color='green')\n",
    "        axes[0, 1].set_title('Episode Lengths')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Steps')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot success rate\n",
    "        axes[1, 0].clear()\n",
    "        if len(success_rates) >= 100:\n",
    "            success_avg = np.convolve(success_rates, np.ones(100)/100, mode='valid')\n",
    "            axes[1, 0].plot(range(99, len(success_rates)), success_avg, color='purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Success Rate (100-ep moving avg)')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Success Rate')\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot curiosity bonuses\n",
    "        axes[1, 1].clear()\n",
    "        if curiosity and len(curiosity_bonuses) > 0:\n",
    "            axes[1, 1].plot(curiosity_bonuses, alpha=0.5, color='orange')\n",
    "            axes[1, 1].set_title('Curiosity Bonuses')\n",
    "        else:\n",
    "            recent_rewards = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
    "            axes[1, 1].hist(recent_rewards, bins=20, alpha=0.7, color='blue')\n",
    "            axes[1, 1].set_title('Recent Reward Distribution')\n",
    "        axes[1, 1].set_xlabel('Episode' if curiosity else 'Reward')\n",
    "        axes[1, 1].set_ylabel('Bonus' if curiosity else 'Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{TRAINING_CONFIG['n_episodes']}\")\n",
    "        print(f\"  Reward: {episode_reward:.2f}\")\n",
    "        print(f\"  Steps: {step}\")\n",
    "        print(f\"  Avg Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "        print(f\"  Success Rate (last 100): {np.mean(success_rates[-100:]):.1%}\")\n",
    "        if curriculum:\n",
    "            print(f\"  Curriculum Difficulty: {curriculum.current_difficulty:.2f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        save_path = f\"{MODEL_DIR}/{agent_type}_ep{episode+1}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'episode': episode + 1,\n",
    "            'config': TRAINING_CONFIG,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'success_rates': success_rates\n",
    "        }\n",
    "        \n",
    "        if agent_type == 'ppo':\n",
    "            checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        elif agent_type == 'dqn':\n",
    "            checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "            checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        elif agent_type == 'mappo':\n",
    "            checkpoint['actor_critic_state_dict'] = agent.actor_critic.state_dict()\n",
    "            checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"\\n‚úì Checkpoint saved: {save_path}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Average Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Final Success Rate (last 100): {np.mean(success_rates[-100:]):.1%}\")\n",
    "print(f\"Best Episode Reward: {max(episode_rewards):.2f}\")\n",
    "print(f\"\\nModels saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffcb94",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final trained model\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "final_path = f\"{MODEL_DIR}/{agent_type}_final_{timestamp}.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    'episode': TRAINING_CONFIG['n_episodes'],\n",
    "    'config': TRAINING_CONFIG,\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'episode_lengths': episode_lengths,\n",
    "    'success_rates': success_rates,\n",
    "    'final_stats': {\n",
    "        'avg_reward': np.mean(episode_rewards[-100:]),\n",
    "        'success_rate': np.mean(success_rates[-100:]),\n",
    "        'best_reward': max(episode_rewards)\n",
    "    }\n",
    "}\n",
    "\n",
    "if agent_type == 'ppo':\n",
    "    checkpoint['network_state_dict'] = agent.network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "elif agent_type == 'dqn':\n",
    "    checkpoint['q_network_state_dict'] = agent.q_network.state_dict()\n",
    "    checkpoint['target_network_state_dict'] = agent.target_network.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "elif agent_type == 'mappo':\n",
    "    checkpoint['actor_critic_state_dict'] = agent.actor_critic.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = agent.optimizer.state_dict()\n",
    "\n",
    "torch.save(checkpoint, final_path)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_path}\")\n",
    "print(f\"\\nTo use this model locally:\")\n",
    "print(f\"1. Download from Google Drive: {MODEL_DIR}\")\n",
    "print(f\"2. Place in your local models/ directory\")\n",
    "print(f\"3. Run: python visualize.py --model models/{agent_type}_final_{timestamp}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977e4c8",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved models\n",
    "import os\n",
    "models = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pt')]\n",
    "models.sort()\n",
    "\n",
    "print(f\"Saved Models ({len(models)}):\")\n",
    "print(\"=\"*80)\n",
    "for i, model in enumerate(models, 1):\n",
    "    path = os.path.join(MODEL_DIR, model)\n",
    "    size = os.path.getsize(path) / (1024 * 1024)  # MB\n",
    "    print(f\"{i}. {model} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüíæ Access models at: {MODEL_DIR}\")\n",
    "print(\"üì• Download from Google Drive to use locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally zip all models for easy download\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "zip_name = f\"autonomous_colony_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "zip_path = f\"/content/{zip_name}\"\n",
    "\n",
    "shutil.make_archive(zip_path, 'zip', MODEL_DIR)\n",
    "\n",
    "print(f\"‚úÖ All models zipped to: {zip_path}.zip\")\n",
    "print(f\"üì• Download this file to get all trained models\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1238f36",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Quick Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of trained agent\n",
    "print(\"Testing trained agent...\\n\")\n",
    "\n",
    "test_env = ColonyEnvironment(n_agents=TRAINING_CONFIG['n_agents'], grid_size=TRAINING_CONFIG['grid_size'])\n",
    "test_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    observations = test_env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done and step < 200:\n",
    "        if agent_type == 'mappo':\n",
    "            actions, _, _ = agent.select_actions(observations, training=False)\n",
    "        else:\n",
    "            actions = []\n",
    "            for obs in observations:\n",
    "                if agent_type == 'ppo':\n",
    "                    action, _, _ = agent.select_action(obs, training=False)\n",
    "                else:\n",
    "                    action = agent.select_action(obs, training=False)\n",
    "                actions.append(action)\n",
    "        \n",
    "        next_observations, rewards, dones, truncated, _ = test_env.step(actions)\n",
    "        episode_reward += sum(rewards)\n",
    "        observations = next_observations\n",
    "        done = truncated or all(dones)\n",
    "        step += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    print(f\"Test Episode {ep + 1}: Reward = {episode_reward:.2f}, Steps = {step}\")\n",
    "\n",
    "print(f\"\\nAverage Test Reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
